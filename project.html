<!doctype html>
<html lang="en">

<head>
	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<link rel="icon" href="img/favicon.png" type="image/png">
	<title>Elements</title>
	<!-- Bootstrap CSS -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<link rel="stylesheet" href="vendors/linericon/style.css">
	<link rel="stylesheet" href="css/font-awesome.min.css">
	<link rel="stylesheet" href="vendors/owl-carousel/owl.carousel.min.css">
	<link rel="stylesheet" href="css/magnific-popup.css">
	<link rel="stylesheet" href="vendors/nice-select/css/nice-select.css">
	<!-- main css -->
	<link rel="stylesheet" href="css/style.css">
</head>

<body class="blog_version">

	<!--================ Start Header Area =================-->
	<header class="header_area">
		<div class="main_menu">
			<nav class="navbar navbar-expand-lg navbar-light">
				<div class="container">
					<!-- Brand and toggle get grouped for better mobile display -->
					<a class="navbar-brand logo_h" href="index.html"><img src="img/logo.png" alt=""></a>
					<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
					 aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					</button>
					<!-- Collect the nav links, forms, and other content for toggling -->
					<div class="collapse navbar-collapse offset" id="navbarSupportedContent">
						<ul class="nav navbar-nav menu_nav justify-content-end">
							<li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
							<li class="nav-item"><a class="nav-link" href="education.html">Education</a></li>
							<li class="nav-item"><a class="nav-link" href="experience.html">Experience</a></li>
							<li class="nav-item"><a class="nav-link" href="publication.html">Publication</a></li>
                            <li class="nav-item"><a class="nav-link" href="project.html">Project</a></li>
							<li class="nav-item"><a class="nav-link" href="award.html">Award</a></li>
							<li class="nav-item"><a class="nav-link" href="event.html">Event</a></li>
							<li class="nav-item submenu dropdown">
								<a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true"
								 aria-expanded="false">Blog</a>
								<ul class="dropdown-menu">
									<li class="nav-item"><a class="nav-link" href="blog.html">Blog</a></li>
									<li class="nav-item"><a class="nav-link" href="single-blog.html">Blog Details</a></li>
								</ul>
							</li>
						</ul>
					</div>
				</div>
			</nav>
		</div>
	</header>
	<!--================ End Header Area =================-->
	
	
    <!--================Start Portfolio Details Area =================-->
	<section class="portfolio_details_area section_gap">
        <div class="container">

			<h3 class="text-heading title_color"><b>1. Surveillance System to Track Individuals Using Gait Biometrics</b></h3>

            <div class="portfolio_details_inner">
                <div class="row">
                    
                    <div class="offset-lg-1 col-lg-5">
                          <div class="portfolio_right_text mt-30">
                              <h5 class="text-uppercase">PhD Project, Manipal University Jaipur, India</h5>
                                   <p>Human gait recognition using the model-free approaches can be done through the analysis of moving shape and motion of the subject's body. The benefit of this approach is that the recognition can be performed at large distance with sufficiently low-resolution images. This approach is very simple and intuitive to extract gait signatures from the gait frames. </p>
                                    <ul class="list">
                                     <li><span>Duration </span>: Jan 2018 – Dec 2022</li>
                                     <li><span>Technology</span>: Deep Learning, Computer Vision, Biometrics (Gait)</li>
                                     <li><span>Application</span>: Surveillance</li>
								     <li><span>Interface</span>: GUI</li>
								     <li><span>Hardware</span>: Jetson Nano</li>
								     <li><span>Dataset</span>: CASIA, TUM, OUISIR</li>
								   
                                     </ul>
                             </div>
			                 </div>
							
							<div class="col-lg-6">
                            <div class="left_img">
                               <img class="img-fluid" src="img/project/4.png" alt="">
							   <div class="button-group-area mt-40"> <br>
								<a href="#" class="genric-btn link circle">Github Link</a>
								<a href="#" class="genric-btn link circle">Research paper Link</a>
								</div>
                            </div>
                        </div>
                 </div>
                


					<div class="row">
						<div class="col-md-3">
							<img src="img/project/1.png" alt="" class="img-fluid">
						</div>
						<div class="col-md-9 mt-sm-20 left-align-p">
							<p>The most interesting research in biometrics is automatic gait recognition when compared to other human unique features. Though there are many approaches to overcome the variations in gait recognition, still there are challenges to recognize a person. Challenges are distinctive gait datasets, degree of stability in identifying, sensing modality, covariates and spoofing effects, and exploring new algorithms. From the developmental perspective, human gait recognition has gained maturity in adopting recent methodologies to provide the high accuracy and the same was analyzed and studied in the view of vision based system. In this paper, the basic knowledge about the human gait is identified and explored. The comparative analysis with the existing techniques of gait recognition were discussed. In this work, the survey of recent deep architecture model on human gait identification, authentication and clinical applications were discussed.</p>
						</div>
					</div>
				
					<div class="row">
						<div class="col-md-9">
							<p class="text-right">The Gait Recognition System (GRS) is a biometric system that is utilized for security. Because
								the computational complexity of GRS is so great, a high-end hardware setup is required. It
								is tough to execute such algorithms on edge devices and if executed on cloud then network
								connectivity and data security issue arises. Furthermore, changes in an individual’s movement
								and wearing clothing, and carrying a bag are key covariate variables that affect a system’s
								performance. Furthermore, with GRS, identification under varied view angles is a significant
								difficulty. In this, a unique, completely automated, and optimal technique for edge devices for
								GRS under various covariates is proposed using deep learning as DeepGait.
								Preprocessing original video frames, creating an optimal deep learning pipeline utilizing the
								CNN model for feature extraction, reducing additional features, hyper tuning the network
								for all covariate circumstances, and ultimately subject detection are the key phrases. The
								extraction of CNN features is a crucial phase to obtain the most functional features. To do this,
								we carefully pick network settings. We created a network after running the datasets numerous
								times and attaining the lowest error rate. In the CASIA B dataset, eleven distinct view angles,
								occlusion in carrying state, and apparel variation are employed in the assessment method. An
								accuracy of 98.64 percent is reached on the jetson nano. Compared to current state-of-the-art
								methodologies, the results indicate a considerable increase in accuracy and runtime on edge
								devices like the Jetson Nano.</p>
							
						</div>
						<div class="col-md-3">
							<img src="img/project/2.png" alt="" class="img-fluid">
						</div>

					</div>
                        
						<div class="row">
							<div class="col-md-4">
								<div class="single-defination">
									<h4 class="mb-20">Application Interface</h4>
									<img src="img/project/3.png" alt="" class="img-fluid">
								</div>
							</div>
							<div class="col-md-4">
								<div class="single-defination">
									<h4 class="mb-20">Accuracy</h4>
									<img src="img/project/5.png" alt="" class="img-fluid">
								</div>
							</div>
							<div class="col-md-4">
								<div class="single-defination">
									<h4 class="mb-20">Results</h4>
									<img src="img/project/6.png" alt="" class="img-fluid">

								</div>
							</div>
						</div>
            </div>



			<div class="section-top-border">
				<br>
				<h3 class="text-heading title_color"><b>2. Face De-Identification Pipeline Based on Physiological & Machine Recognition Experiments Using Deep Learning</b></h3>
				<div class="portfolio_details_inner">
					<div class="row">
						<div class="offset-lg-1 col-lg-5">
							  <div class="portfolio_right_text mt-30">
								  <h5 class="text-uppercase">Croatian Government funded project with with Prof. Dr. Scientist. Slobodan Ribarić, Zagreb Croatia</h5>
									   <p>We proposed a reversible face de identification pipeline that modifies face geometry and texture. Fourteen parameters for geometrical modification are used. For texture modification fixed face texture template is used. </p>
										<ul class="list">
										 <li><span>Duration </span>: 5 Sept 2016 – 30 Dec 2018</li>
										 <li><span>Technology</span>: Deep Learning, Computer Vision, Biometrics (Face)</li>
										 <li><span>Application</span>: Surveillance, De-Identification</li>
										 <li><span>Hardware</span>: Jetson Nano</li>
										 <li><span>Dataset</span>: Famoud faces</li>
										 </ul>
								</div>
						  </div>
     							<div class="col-lg-6">
								<div class="left_img">
								   <img class="img-fluid" src="img/project/9.png" alt="">
								   <div class="button-group-area mt-40"> <br>
									<a href="#" class="genric-btn link circle">Github Link</a>
									<a href="#" class="genric-btn link circle">Research paper Link</a>
									</div>
								</div>
							</div>
					 </div>
					          <div class="row">
							      <div class="col-md-3"><video width="280" height="240" controls><source src="img/project/1.mp4" type="video/mp4"></video></div>
							      <div class="col-md-9 mt-sm-20 left-align-p">
							     	<p>We compiled a set of 30 face images of famous people (7 females and 23 males with ages ranging from 30 to 75) from politics, sports, business and entertainment. The used testing procedure is like the one used for diagnosing of the prosopagnosia a neurological disorder characterized by the inability to recognize human faces. Images of de-identified faces of famous persons are presented to the test subjects with a request to try to recognize them. Obtained answers are recorded for later matching with ground truth answers. The main aim of the performed testing is to evaluate the impact of geometrical and texture modifications on human ability to recognize faces. The evaluation is performed by means of crowdsourcing performed by 150 test subjects (20 females and 130 males). The test subjects were informed that faces in the tests are de-identified faces of famous people. The background (ie context) and biometrical ques like hair and ears, that a user can use for face identification are removed in all tests.</p>
							       </div>
						       </div>
				 </div>
							
							<div class="row">
								<div class="col-md-4">
									<div class="single-defination">
										<h4 class="mb-20">Results</h4>
										<p> We have investigated impact of various geometrical and texture modifications of face components like cyes, eyebrows, nose and lips on ability of humans and machines to recognize faces. The crowdsourcing and machine face recognition experiments were performed on images of famous people collected from the Internet. The obtained results in both types of experiments showed that face texture has stronger impact on a level of privacy protection then face geometry shape) modifications. </p>
									</div>
								</div>
								<div class="col-md-4">
									<div class="single-defination">
										<h4 class="mb-20">Application Interface</h4>
										<img src="img/project/7.jpeg" alt="" class="img-fluid">
									</div>
								</div>
								<div class="col-md-4">
									<div class="single-defination">
										<h4 class="mb-20">face Dataset</h4>
										<img src="img/project/8.png" alt="" class="img-fluid">
	
									</div>
								</div>
							</div>
				</div>
	


				









				<div class="section-top-border">
	<br>
					<h3 class="text-heading title_color"><b>3. Classification Of Gait Data Using Machine Learning Techniques To Categorise Human Locomotion</b></h3>
				
					<div class="portfolio_details_inner">
						<div class="row">
							
							<div class="offset-lg-1 col-lg-5">
								  <div class="portfolio_right_text mt-30">
									  <h5 class="text-uppercase">M.Tech Project, VCE Rohtak</h5>
										   <p>Shown the importance of gait recognition in order to detect whether a human GAIT is normal gait or pathological gait.  </p>
											<ul class="list">
											 <li><span>Duration </span>: 5 Sept 2014 – 30 June 2016

											 </li>
											 <li><span>Technology</span>: Machine Learning, Computer Vision

											 </li>
											 <li><span>Application</span>: Medical, Early detection</li>
											 <li><span>Hardware</span>: Jetson Nano</li>
											 <li><span>Dataset</span>: Crouch dataset. 
											 </li>
										   
											 </ul>
									 </div>
									 </div>
									
									<div class="col-lg-6">
									<div class="left_img">
									   <img class="img-fluid" src="img/project/10.png" alt="">
									   <div class="button-group-area mt-40"> <br>
										<a href="#" class="genric-btn link circle">Github Link</a>
										<a href="#" class="genric-btn link circle">Research paper Link</a>
										</div>
									</div>
								</div>
						 </div>
						
		
		
							<div class="row">
								<video width="280" height="240" controls>
									<source src="img/project/2.mp4" type="video/mp4">
									</video>
								<div class="col-md-9 mt-sm-20 left-align-p">
									<p>Three main approaches in gait classification i.e. back propagation and KNN. After comparing the testing and training results we get better results using back propagation learning technique. The whole work of paper is to describe the classification technique of different type of GAIT into following four categories: Normal, crouch2, crouch3, crouch4 using back propagation and KNN. When the training of data is done then the output of will be either four options i.e. Normal, crouch2, crouch3, crouch4. If the testing data set is tested on the proposed system then the output must be normal GAIT if the classifier classifies it as normal data or the output will be abnormal GAIT data if the classifier classifies it as crouch2, crouch3, crouch4 in any of these three categories.										.</p>
								</div>
							</div>
						
							
								
								<div class="row">
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Application Interface</h4>
											<img src="img/project/12.png" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Accuracy</h4>
											<img src="img/project/11.png" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Results</h4>
											<p>So we first select the feature and identify the principal feature then we classify gait data and use different machine learning techniques (K-mean, KNN and Back Propagation) and performance comparison is shown. Experimental results on real time datasets proposed method is better than previous method as far as humanoid locomotion classification is concerned.</p>
		
										</div>
									</div>
								</div>
					</div>
		
				</div>
	
	






					<div class="section-top-border">

					<h3 class="text-heading title_color"><b>4. Low-cost IoT enabled Board Marker using Image Processing</b></h3>

					<div class="portfolio_details_inner">
						<div class="row">
							
							<div class="offset-lg-1 col-lg-5">
								  <div class="portfolio_right_text mt-30">
									  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
										   <p>This project is based on robotics. Robots have been easing human tasks since decades and in addition to the numerous tasks they perform for us, this time its desired that they contribute towards modern education. </p>
											<ul class="list">
											 <li><span>Duration </span>: Jan 2019 – Dec 2020</li>
											 <li><span>Technology</span>: Image Processing, IoT</li>
											 <li><span>Application</span>: Automation, class room teaching

											 </li>
											 <li><span>Interface</span>: GUI - Processing Software version 2.2</li>
											 <li><span>Hardware</span>: Arduino mega, 9V DC supply, Two Stepper Motors, One Servo Motor, Male-Male, Male-Female and Female-Female Jumper Cables, 16 Teeth Pulley Cable, Drawing Board, Drawing Instrument (e.g. Marker), L293D Motor Shield, Heat Sinks, Weights</li>
											 <li><span>Dataset</span>: JPG/PNG and SVG format 

											 </li>
										   
											 </ul>
									 </div>
									 </div>
									
									<div class="col-lg-6">
									<div class="left_img">
									   <img class="img-fluid" src="img/project/13.jpeg" alt="">
									   <div class="button-group-area mt-40"> <br>
										<a href="#" class="genric-btn link circle">Github Link</a>
										<a href="#" class="genric-btn link circle">Research paper Link</a>
										</div>
									</div>
								</div>
						 </div>
						
		
		
							<div class="row">
								<video width="280" height="240" controls>
									<source src="img/project/3.mp4" type="video/mp4">
									</video>
								<div class="col-md-9 mt-sm-20 left-align-p">
									<p>The project would be implemented keeping the teaching industry in mind but could also find its applications in areas that require designing. The basic idea of this project is that a robot would be fed a reference drawing, mostly text-based input or a diagram, which it would then replicate on an output area of our choice for example a white board or a notebook. The benefits of such a robot would be that it would save time that would be required to produce a complex and detailed diagram on a white board for teaching purposes and quite obviously be helpful in graphic designing on materials of our choice if implemented and refined further. The system works on a pulley system where an end-effector is manipulated using the pulleys. The application running on the PC is fed with the image which the recognizes the edges of the image and then sends command numbers to the microcontroller. The microcontroller upon receiving the command numbers performs commands associated with the received command numbers. The microcontroller rotates the stepper motor as per the need of the image to be drawn, the servo motor attached at the pen-holder acts as a pen lifting mechanism similar to the action we perform while writing or drawing. The pulley system adds to the maneuverability of the system while cutting down on costs. The system works as per predicted and produces good quality drawings on the drawing pad.</p>
								</div>
							</div>
						
							
								
								<div class="row">
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Application Interface</h4>
											<img src="img/project/14.jpeg" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Accuracy</h4>
											<img src="img/project/15.png" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Results</h4>
											<p>With the 9V of DC supply, the stepper motors run smoothly with no jitters. The circuit doesn’t experience overheating at this power supply. The systems performance is heavily dependent on the power supply. The pen-lift actions are fluid and precise and the system responds quickly to the drawing commands somewhere between 0.5-1 second.</p>
		
										</div>
									</div>
								</div>
					</div>
		
		
				</div>
		
		
		
		







					<div class="section-top-border">

					<h3 class="text-heading title_color"><b>5. IoT Enabled Mechanical Chess based on Artificial Intelligence</b></h3>

					<div class="portfolio_details_inner">
						<div class="row">
							
							<div class="offset-lg-1 col-lg-5">
								  <div class="portfolio_right_text mt-30">
									  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
										   <p>Automated AI chess board is a physical chess board which lets the player play against an AI the chess board is powered by raspberry Pi. </p>
											<ul class="list">
											 <li><span>Duration </span>: Jan 2018 – Dec 2019</li>
											 <li><span>Technology</span>: Artificial Intelligence, Robotics, IoT</li>
											 <li><span>Application</span>: Playing chess on a real chess board against machine
											 </li>
											 <li><span>Interface</span>: Raspberry Pi, L298n Motor drivers </li>
											 <li><span>Hardware</span>: Flexible Shaft Coupling, Stainless Steel Lead Screw Rod with Nut, Pillow Block Bearing, box/wood/table, servo motor, raspberry pi, arduino, Chess board, Magnets, Electromagnets, Spools, Belts</li>
											 <li><span>Dataset</span>: Stockfish 

											 </li>
										   
											 </ul>
									 </div>
									 </div>
									
									<div class="col-lg-6">
									<div class="left_img">
									   <img class="img-fluid" src="img/project/16.jpeg" alt="">
									   <div class="button-group-area mt-40"> <br>
										<a href="#" class="genric-btn link circle">Github Link</a>
										<a href="#" class="genric-btn link circle">Research paper Link</a>
										</div>
									</div>
								</div>
						 </div>
						
		
		
							<div class="row">
								<video width="280" height="240" controls>
									<source src="img/project/4.mp4" type="video/mp4">
									</video>
								<div class="col-md-9 mt-sm-20 left-align-p">
									<p>The Field of Artificial Intelligence is very vast but the prime goal of researchers and engineers is to enable the computer to think like a human. And a game of chess require a lot of thinking and logic, for a long time we have been playing the game of chess to exercise our brains and entertain ourselves at the same time. And we all love the feel of an original chess board. It would be really nice to play the game of chess against an Physical Board.

										Using the core XY motion of two motors we are enabling an electromagnet to span a whole chess board and the electromagnet will be responsible to slide the pieces across the whole board. Using two Nema 17 motors and two lead screw mechanism for our X and Y axises both powered by L2N8 motor drivers and a relay module to control the electromagnet the chess pieces with sufficiently strong magnets attached to the bottom will slide across the board. Using the Min-Max spanning tree providing an initial score to each player the goal of our AI is to minimize the score of the opposing player and trying to maintain its own score.
										
										User will make a move and depending on the move made by user our AI will generate a move tree containing possible moves. Every piece of the board has a rank and a score which is equal to the sum of all the ranks of alive pieces of that particular color will be assigned at each step to the color. Each node of the move tree contains the score corresponding to the move, treating scores as weights of the tree we apply Min-Max search to make the best possible move.</p>
								</div>
							</div>
						
							
								
								<div class="row">
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Application Interface</h4>
											<img src="img/project/17.png" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Results</h4>
											<img src="img/project/18.png" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Results</h4>
											<p>Tests were performed with certain parameters in mind Speed of AI: The AI performed above expectations in terms of speed. X motion track and motor: It ran fast and delivered accurately but made a lot of noise, indicating that it was not perfectly smooth. Y motion track and motor: It was smoother than the X track but was slightly slower due to the extra weight and added friction from the optical axis shafts. Electromagnet: It was stronger than expected and due to unevenness of board occasionally lost pieces, this was corrected by reducing the height of the mount on which it rested and increasing the voltage to give more power. We used a new approach to core XY motion which efficiently handles large weight while still maintaining the speed. The AI developed for our project is fast and very reliable for low end embedded systems.</p>
		
										</div>
									</div>
								</div>
					</div>
		
		
				</div>
		
		
		
		







					<div class="section-top-border">

					<h3 class="text-heading title_color"><b>6. Optimized Navigation using Deep Learning Technique for Automatic Guided Vehicle</b></h3>

					<div class="portfolio_details_inner">
						<div class="row">
							
							<div class="offset-lg-1 col-lg-5">
								  <div class="portfolio_right_text mt-30">
									  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
										   <p>This study tackled the mentioned problems with a straightforward and cost-effective solution, using end to end learning and replacing the numerous sensors with a camera and commandeering just the forward, backward, left, and right controls.</p>
											<ul class="list">
											 <li><span>Duration </span>: June 2018 – Dec 2019</li>
											 <li><span>Technology</span>: Deep Learning, IoT, Computer Vision</li>
											 <li><span>Application</span>: Smart car, Autonomous, Detect Stop Signs and Traffic Lights and Avoid Front Collision.



											 </li>
											 <li><span>Software</span>: Raspbian OS, 2. Python, OpenCV Libraries</li>
											 <li><span>Hardware</span>: RC Car, Raspberry Pi,  Ultrasonic sensor, PI Camera </li>
											 <li><span>Dataset</span>: Stimulator for autonomous car by Udacity generated training data



											 </li>
										   
											 </ul>
									 </div>
									 </div>
									
									<div class="col-lg-6">
									<div class="left_img">
									   <img class="img-fluid" src="img/project/19.png" alt="">
									   <div class="button-group-area mt-40"> <br>
										<a href="#" class="genric-btn link circle">Github Link</a>
										<a href="#" class="genric-btn link circle">Research paper Link</a>
										</div>
									</div>
								</div>
						 </div>
						
		
		
							<div class="row">
								<video width="280" height="240" controls>
									<source src="img/project/5.mp4" type="video/mp4">
									</video>
								<div class="col-md-9 mt-sm-20 left-align-p">
									<p>Autonomous driving has passed the point of being called the biggest step, as the smart car revolution is already taking shape around the world. Self-driving cars are relevant if not prevalent and the biggest obstacles to reach the mass adoption are customer acceptance, cost, infrastructure and the reliance on several onerous algorithms that include perception, lane marking detection, path planning and variation in pathways. In this research, authors have used most popular method of deep learning i.e. Convolution Neural Network (CNN) to train the collected data on the VGG16 model. Later these have optimized directly by the proposed system with cropping each unnecessary image and mapping pixels from a single front-facing camera to direct steering instructions. It has been observed from the experimental work that proposed model has given better result than the existing work i.e. increase in the accuracy from 88% (Udacity training dataset) to 98% (proposed). This model is suitable for industrial use and robust in real time scenarios therefore can be applied in modern industrialized systems.</p>
								</div>
							</div>
						
							
								
								<div class="row">
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Application Interface</h4>
											<img src="img/project/20.jpeg" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Results</h4>
											<img src="img/project/21.png" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Accuracy</h4>
											<p>After rigorous test runs on tracks apart from which the car is trained on, it is concluded that the car can function competently in a controlled environment. The neural network is also working fittingly and gave 98% results.</p>
		
										</div>
									</div>
								</div>
					</div>
		
		
				</div>
		
		
		





					<div class="section-top-border">

						<h3 class="text-heading title_color"><b>7. IoT based Smart Assistance Spoon for Parkinson Patients</b></h3>
	
						<div class="portfolio_details_inner">
							<div class="row">
								
								<div class="offset-lg-1 col-lg-5">
									  <div class="portfolio_right_text mt-30">
										  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
											   <p>Our primary objective is to make the Robotic arm, having 4 servo motors to interface with the development of a microcontroller based Robotic arm.</p>
												<ul class="list">
												 <li><span>Duration </span>: Jan 2017 – Dec 2018</li>
												 <li><span>Technology</span>: Artificial Intelligence, Robotics, IoT</li>
												 <li><span>Application</span>: Feed the patient with no efforts needed

												 </li>
												 <li><span>Interface</span>: Android App</li>
												 <li><span>Hardware</span>: Raspberry Pi, Servo Motors, Stepper motor, Robotic arm, Breadboard, Jumper Cables, Other modules like Bluetooth module, WIFI module </li>
												 <li><span>Dataset</span>:  Patient height


	
	
	
												 </li>
											   
												 </ul>
										 </div>
										 </div>
										
										<div class="col-lg-6">
										<div class="left_img">
										   <img class="img-fluid" src="img/project/22.png" alt="">
										   <div class="button-group-area mt-40"> <br>
											<a href="#" class="genric-btn link circle">Github Link</a>
											<a href="#" class="genric-btn link circle">Research paper Link</a>
											</div>
										</div>
									</div>
							 </div>
							
			
			
								<div class="row">
									<video width="280" height="240" controls>
										<source src="img/project/6.mp4" type="video/mp4">
										</video>
									<div class="col-md-9 mt-sm-20 left-align-p">
										<p>The arm control by robotics is very popular in the world of robotics. The essential part of the robotic arm is a programmable micro controller-based brick capable of driving basically four servos to form an anthropomorphic structure.

											Our primary objective is to make the Robotic arm, having 4 servo motors to interface with the development of a microcontroller based Robotic arm. It provides more interfaces to the outside world and has larger memory to store many programs.
											
											The technology for assisting people who are functionally challenged has improved over the recent decades. A group that suffer from this ailment are people with Parkinson’s disease. Parkinson’s disease (PD) is a neurological degenerative disease that causes uncontrollable shaking and makes it difficult for the affected person to eat. There is no cure for PD, but there is technology and potential for new technology that can help people who carry the disease with their daily lives.
											
											The machine will be of great use to perform repetitive tasks of picking and placing of small edibles upto 500 grams in one serving.
											
											It can be used to do small assembly work effectively due to its great added accuracy for placement purpose, which has further extended scope of our project.</p>
									</div>
								</div>
							
								
									
									<div class="row">
										<div class="col-md-4">
											<div class="single-defination">
												<h4 class="mb-20">Application Interface</h4>
												<img src="img/project/23.jpeg" alt="" class="img-fluid">
											</div>
										</div>
										<div class="col-md-4">
											<div class="single-defination">
												<h4 class="mb-20">Results</h4>
												<img src="img/project/24.jpeg" alt="" class="img-fluid">
											</div>
										</div>
										<div class="col-md-4">
											<div class="single-defination">
												<h4 class="mb-20">Accuracy</h4>
												<p>The robotic arm model was successfully implemented which was able to pick up food from the tray and direct it towards the mouth of the user. The Bluetooth app for connecting to the arm, controlling and terminating the process was also developed.</p>
			
											</div>
										</div>
									</div>
						</div>
			
					</div>
					











				
			<div class="section-top-border">
				<br>
				<h3 class="text-heading title_color"><b>8. Unmanned Aerial Vehicle for Surveillance</b></h3>
				<div class="portfolio_details_inner">
					<div class="row">
						<div class="offset-lg-1 col-lg-5">
							  <div class="portfolio_right_text mt-30">
								  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
									   <p>Drone is an all-purpose medical toolkit that can be automatically flown to any emergency situation and used to guide citizens to make non-technical lifesaving procedures.</p>
										<ul class="list">
										 <li><span>Duration </span>: Jan 2020 – Present</li>
										 <li><span>Technology</span>: Artificial Intelligence, Robotics, IoT</li>
										 <li><span>Application</span>: Surveillance</li>
										 <li><span>Interface</span>: Controlling quadcopter with mobile application</li>
										 <li><span>Hardware</span>: Quadcopter, Battery, Ardupilot, ESCs(electronic speed controllers), Gyroscope, wifi module, Installing camera with quadcopter, Inserting microphone in drone, Interfacing different Sensors</li>
										 <li><span>Dataset</span>: Environment

										 </li>
										 </ul>
								</div>
						  </div>
     							<div class="col-lg-6">
								<div class="left_img">
								   <img class="img-fluid" src="img/project/23.png" alt="">
								   <div class="button-group-area mt-40"> <br>
									<a href="#" class="genric-btn link circle">Github Link</a>
									<a href="#" class="genric-btn link circle">Research paper Link</a>
									</div>
								</div>
							</div>
					 </div>
					          <div class="row">
							      <div class="col-md-3"><video width="280" height="240" controls><source src="img/project/7.mp4" type="video/mp4"></video></div>
							      <div class="col-md-9 mt-sm-20 left-align-p">
							     	<p>When a medical emergency takes place in natural flood, the response time can make all the difference between a life saved and a life lost. Unfortunately, ambulances can’t reach in the place due to over water flooding, in which time a cardiac arrest victim may have already succumbed to a lack of oxygen to the brain. Drone is an all-purpose medical toolkit that can be automatically flown to any emergency situation and used to guide citizens to make non-technical lifesaving procedures.

										With our drone we dramatically increase this survival rate. The incorporation of a two-way, video supported, communication channel in the drone between operators and the first responders will improve first care. Successful drone usage by lay-persons is currently at 20%. With personalized instructions and communication on the drone, this can be increased to 90%. In short, the drone helps to save lives by extending existing emergency infrastructure with a network of fast and compressed video broadcasting channel capable of bringing emergency supplies and establishing communication between dedicated source and destination.</p>
							       </div>
						       </div>
				 </div>
							
							<div class="row">
								<div class="col-md-4">
									<div class="single-defination">
										<h4 class="mb-20">Results</h4>
										<p>In this project, we are building a drone which can provide live video coverage for monitoring purpose. Additionally, GPS module can also be used. GPS module will give us the location of the drone. Then, after attaching telemetry device, waypoints can be given to drone so that it can automatically go to the required location and come back to the same place from where it was launched.</p>
									</div>
								</div>
								<div class="col-md-4">
									<div class="single-defination">
										<h4 class="mb-20">Application Interface</h4>
										<img src="img/project/26.jfif" alt="" class="img-fluid">
									</div>
								</div>
								<div class="col-md-4">
									<div class="single-defination">
										<h4 class="mb-20">Hardware Interface</h4>
										<img src="img/project/25.jpg" alt="" class="img-fluid">
	
									</div>
								</div>
							</div>
				</div>
		











				
				<div class="section-top-border">
					<br>
					<h3 class="text-heading title_color"><b>9. Smartphone-based Sleep Staging using 1-Channel EEG</b></h3>
					<div class="portfolio_details_inner">
						<div class="row">
							<div class="offset-lg-1 col-lg-5">
								  <div class="portfolio_right_text mt-30">
									  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
										   <p>Drone is an all-purpose medical toolkit that can be automatically flown to any emergency situation and used to guide citizens to make non-technical lifesaving procedures.</p>
											<ul class="list">
											 <li><span>Duration </span>: Jan 2020 – Present</li>
											 <li><span>Technology</span>: Artificial Intelligence, Robotics, IoT</li>
											 <li><span>Application</span>: Surveillance</li>
											 <li><span>Interface</span>: Controlling quadcopter with mobile application</li>
											 <li><span>Hardware</span>: Quadcopter, Battery, Ardupilot, ESCs(electronic speed controllers), Gyroscope, wifi module, Installing camera with quadcopter, Inserting microphone in drone, Interfacing different Sensors</li>
											 <li><span>Dataset</span>: Environment
	
											 </li>
											 </ul>
									</div>
							  </div>
									 <div class="col-lg-6">
									<div class="left_img">
									   <img class="img-fluid" src="img/project/23.png" alt="">
									   <div class="button-group-area mt-40"> <br>
										<a href="#" class="genric-btn link circle">Github Link</a>
										<a href="#" class="genric-btn link circle">Research paper Link</a>
										</div>
									</div>
								</div>
						 </div>
								  <div class="row">
									  <div class="col-md-3"><video width="280" height="240" controls><source src="img/project/7.mp4" type="video/mp4"></video></div>
									  <div class="col-md-9 mt-sm-20 left-align-p">
										 <p>When a medical emergency takes place in natural flood, the response time can make all the difference between a life saved and a life lost. Unfortunately, ambulances can’t reach in the place due to over water flooding, in which time a cardiac arrest victim may have already succumbed to a lack of oxygen to the brain. Drone is an all-purpose medical toolkit that can be automatically flown to any emergency situation and used to guide citizens to make non-technical lifesaving procedures.
	
											With our drone we dramatically increase this survival rate. The incorporation of a two-way, video supported, communication channel in the drone between operators and the first responders will improve first care. Successful drone usage by lay-persons is currently at 20%. With personalized instructions and communication on the drone, this can be increased to 90%. In short, the drone helps to save lives by extending existing emergency infrastructure with a network of fast and compressed video broadcasting channel capable of bringing emergency supplies and establishing communication between dedicated source and destination.</p>
									   </div>
								   </div>
					 </div>
								
								<div class="row">
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Results</h4>
											<p>In this project, we are building a drone which can provide live video coverage for monitoring purpose. Additionally, GPS module can also be used. GPS module will give us the location of the drone. Then, after attaching telemetry device, waypoints can be given to drone so that it can automatically go to the required location and come back to the same place from where it was launched.</p>
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Application Interface</h4>
											<img src="img/project/26.jfif" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Hardware Interface</h4>
											<img src="img/project/25.jpg" alt="" class="img-fluid">
		
										</div>
									</div>
								</div>
					</div>
			





				
					<div class="section-top-border">
						<br>
						<h3 class="text-heading title_color"><b>10. Smartphone-based Sleep Staging using 1-Channel EEG</b></h3>
						<div class="portfolio_details_inner">
							<div class="row">
								<div class="offset-lg-1 col-lg-5">
									  <div class="portfolio_right_text mt-30">
										  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
											   <p>Drone is an all-purpose medical toolkit that can be automatically flown to any emergency situation and used to guide citizens to make non-technical lifesaving procedures.</p>
												<ul class="list">
												 <li><span>Duration </span>: Jan 2020 – Present</li>
												 <li><span>Technology</span>: Artificial Intelligence, Robotics, IoT</li>
												 <li><span>Application</span>: Surveillance</li>
												 <li><span>Interface</span>: Controlling quadcopter with mobile application</li>
												 <li><span>Hardware</span>: Quadcopter, Battery, Ardupilot, ESCs(electronic speed controllers), Gyroscope, wifi module, Installing camera with quadcopter, Inserting microphone in drone, Interfacing different Sensors</li>
												 <li><span>Dataset</span>: Environment
		
												 </li>
												 </ul>
										</div>
								  </div>
										 <div class="col-lg-6">
										<div class="left_img">
										   <img class="img-fluid" src="img/project/23.png" alt="">
										   <div class="button-group-area mt-40"> <br>
											<a href="#" class="genric-btn link circle">Github Link</a>
											<a href="#" class="genric-btn link circle">Research paper Link</a>
											</div>
										</div>
									</div>
							 </div>
									  <div class="row">
										  <div class="col-md-3"><video width="280" height="240" controls><source src="img/project/7.mp4" type="video/mp4"></video></div>
										  <div class="col-md-9 mt-sm-20 left-align-p">
											 <p>When a medical emergency takes place in natural flood, the response time can make all the difference between a life saved and a life lost. Unfortunately, ambulances can’t reach in the place due to over water flooding, in which time a cardiac arrest victim may have already succumbed to a lack of oxygen to the brain. Drone is an all-purpose medical toolkit that can be automatically flown to any emergency situation and used to guide citizens to make non-technical lifesaving procedures.
		
												With our drone we dramatically increase this survival rate. The incorporation of a two-way, video supported, communication channel in the drone between operators and the first responders will improve first care. Successful drone usage by lay-persons is currently at 20%. With personalized instructions and communication on the drone, this can be increased to 90%. In short, the drone helps to save lives by extending existing emergency infrastructure with a network of fast and compressed video broadcasting channel capable of bringing emergency supplies and establishing communication between dedicated source and destination.</p>
										   </div>
									   </div>
						 </div>
									
									<div class="row">
										<div class="col-md-4">
											<div class="single-defination">
												<h4 class="mb-20">Results</h4>
												<p>In this project, we are building a drone which can provide live video coverage for monitoring purpose. Additionally, GPS module can also be used. GPS module will give us the location of the drone. Then, after attaching telemetry device, waypoints can be given to drone so that it can automatically go to the required location and come back to the same place from where it was launched.</p>
											</div>
										</div>
										<div class="col-md-4">
											<div class="single-defination">
												<h4 class="mb-20">Application Interface</h4>
												<img src="img/project/26.jfif" alt="" class="img-fluid">
											</div>
										</div>
										<div class="col-md-4">
											<div class="single-defination">
												<h4 class="mb-20">Hardware Interface</h4>
												<img src="img/project/25.jpg" alt="" class="img-fluid">
			
											</div>
										</div>
									</div>
						</div>
				
	





















        </div>
    </section>
    <!--================End Portfolio Details Area =================-->


	
  































          
    <!--================Footer Area =================-->
	<footer class="footer_area">
        <div class="container">
            <div class="row justify-content-center">
                <div class="col-lg-12">
                    <div class="footer_top flex-column">
                        <div class="footer_logo">
                            <a href="#">
                                <i class="fa fa-chevron-up" aria-hidden="true"></i>
                            </a>
                            <h4>Follow Me</h4>
                        </div>
                        <div class="footer_social">
                            <a href="https://jaipur.manipal.edu/foe/schools-faculty/faculty-list/Anubha-parashar.html" target="_blank"><i class="fa fa-university" aria-hidden="true"></i></a>
							<a href="https://www.linkedin.com/in/anubhaparashar/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
							<a href="https://github.com/anubhaparashar" target="_blank"><i class="fa fa-git" aria-hidden="true"></i></a>
							<a href="https://www.facebook.com/anubha.parashar/" target="_blank"><i class="fa fa-facebook"></i></a>
							<a href="https://www.instagram.com/anubha_parashar/" target="_blank"><i class="fa fa-instagram" aria-hidden="true"></i></i></a>
							<a href="https://twitter.com/parashar_anubha" target="_blank"><i class="fa fa-twitter"></i></a>
							<a href="https://www.youtube.com/channel/UCmC85DV_GWSe3scjD-PcRhg" target="_blank"><i class="fa fa-youtube" aria-hidden="true"></i></a>
							<a href="https://in.pinterest.com/anubhaparashar1025/" target="_blank"><i class="fa fa-pinterest-square" aria-hidden="true"></i> </a>
							<a href="live:a199d70eb5cb2d04" target="_blank"><i class="fa fa-skype" aria-hidden="true"></i> live:a199d70eb5cb2d04</a>
							
							

							

                           
							
							<a href="https://scholar.google.com/citations?user=hrwpIAgAAAAJ&hl=en" target="_blank"><img src="img/icons/1.png" alt=""> </a>
							
                            <br>
							
							<a href="https://www.scopus.com/authid/detail.uri?authorId=57191284351" target="_blank"><img src="img/icons/2.png" alt=""></a>

							<a href="https://orcid.org/my-orcid?orcid=0000-0002-8474-3623" target="_blank"><img src="img/icons/3.png" alt=""></a>
							
							
							
							<a href="https://www.webofscience.com/wos/author/record/L-7545-2017" target="_blank"><img src="img/icons/4.png" alt=""></a>
							<a href="https://medium.com/@anubhaparashar1025" target="_blank"><img src="img/icons/5.png" alt=""></a>
							<a href="https://www.semanticscholar.org/author/Anubha-Parashar/2714843" target="_blank"><img src="img/icons/6.png" alt=""></a>
							<a href="https://muj.academia.edu/AnubhaParashar" target="_blank"><img src="img/icons/7.png" alt=""></a>
							
							
							<a href="https://www.researchgate.net/profile/Anubha-Parashar" target="_blank"><img src="img/icons/8.png" alt=""></a>
							
                            <a href="https://anubhaparashar.blogspot.com/" target="_blank"><img src="img/icons/9.png" alt=""></a>
							<a href="http://cooltechnoupdates.blogspot.com/" target="_blank"><img src="img/icons/9.png" alt=""></a>
							<a href="https://cool-computer-tricks-n-tips.blogspot.com/" target="_blank"><img src="img/icons/9.png" alt=""></a>
							
							
							<br>
							
							<a href="mailto:dranubhaparashar@gmail.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i> dranubhaparashar@gmail.com</a>
							<a href="mailto:anubhaparashar1025@gmail.com"- target="_blank"><i class="fa fa-envelope-o" aria-hidden="true"></i> anubhaparashar1025@gmail.com </a>
							<a href="mailto:anubha.parashar@jaipur.manipal.edu" target="_blank"><i class="fa fa-envelope-square" aria-hidden="true"></i> anubha.parashar@jaipur.manipal.edu </a>


							

							
							
                        </div>
                    </div>
                </div>
            </div>
            <div class="row footer_bottom justify-content-center">
                <p class="col-lg-8 col-sm-12 footer-text">
                   
            </div>
        </div>
    </footer>
    <!--================End Footer Area =================-->
    
		   
		<!-- Optional JavaScript -->
		<!-- jQuery first, then Popper.js, then Bootstrap JS -->
		<script src="js/jquery-3.2.1.min.js"></script>
		<script src="js/popper.js"></script>
		<script src="js/bootstrap.min.js"></script>
		<script src="js/stellar.js"></script>
		<script src="vendors/lightbox/simpleLightbox.min.js"></script>
		<script src="vendors/nice-select/js/jquery.nice-select.min.js"></script>
		<script src="vendors/isotope/imagesloaded.pkgd.min.js"></script>
		<script src="vendors/isotope/isotope-min.js"></script>
		<script src="vendors/owl-carousel/owl.carousel.min.js"></script>
		<script src="js/jquery.ajaxchimp.min.js"></script>
		<script src="js/mail-script.js"></script>
		<script src="js/theme.js"></script>
	</body>
</html>