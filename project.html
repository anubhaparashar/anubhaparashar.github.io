<!doctype html>
<html lang="en">

<head>
	<!-- Required meta tags -->
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<link rel="icon" href="img/favicon.png" type="image/png">
	<title>Elements</title>
	<!-- Bootstrap CSS -->
	<link rel="stylesheet" href="css/bootstrap.css">
	<link rel="stylesheet" href="vendors/linericon/style.css">
	<link rel="stylesheet" href="css/font-awesome.min.css">
	<link rel="stylesheet" href="vendors/owl-carousel/owl.carousel.min.css">
	<link rel="stylesheet" href="css/magnific-popup.css">
	<link rel="stylesheet" href="vendors/nice-select/css/nice-select.css">
	<!-- main css -->
	<link rel="stylesheet" href="css/style.css">
</head>

<body class="blog_version">

	<!--================ Start Header Area =================-->
	<header class="header_area">
		<div class="main_menu">
			<nav class="navbar navbar-expand-lg navbar-light">
				<div class="container">
					<!-- Brand and toggle get grouped for better mobile display -->
					<a class="navbar-brand logo_h" href="index.html"><img src="img/logo.png" alt=""></a>
					<button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
					 aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					</button>
					<!-- Collect the nav links, forms, and other content for toggling -->
					<div class="collapse navbar-collapse offset" id="navbarSupportedContent">
						<ul class="nav navbar-nav menu_nav justify-content-end">
							<li class="nav-item"><a class="nav-link" href="index.html">Home</a></li>
							<li class="nav-item"><a class="nav-link" href="education.html">Education</a></li>
							<li class="nav-item"><a class="nav-link" href="experience.html">Experience</a></li>
							<li class="nav-item"><a class="nav-link" href="publication.html">Publication</a></li>
                            							<li class="nav-item submenu dropdown">
								<a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true"
								 aria-expanded="false">Project</a>
								<ul class="dropdown-menu">
                                    <li class="nav-item"><a class="nav-link" href="project.html">Project</a></li>
                                    <li class="nav-item"><a class="nav-link" href="grant.html">Grant</a></li>
								</ul>
								
							</li>
							<li class="nav-item"><a class="nav-link" href="award.html">Award</a></li>
							<li class="nav-item"><a class="nav-link" href="event.html">Event</a></li>
							<li class="nav-item submenu dropdown">
								<a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true"
								 aria-expanded="false">Blog</a>
								<ul class="dropdown-menu">
									<li class="nav-item"><a class="nav-link" href="blog.html">Blog</a></li>
									<li class="nav-item"><a class="nav-link" href="single-blog.html">Blog Details</a></li>
								</ul>
							</li>
						</ul>
					</div>
				</div>
			</nav>
		</div>
	</header>
	<!--================ End Header Area =================-->
	
	
    <!--================Start Portfolio Details Area =================-->
	<section class="portfolio_details_area section_gap">
        <div class="container">

			<h3 style="color:MediumBlue "class="text-heading title_color">1. Surveillance System to Track Individuals Using Gait Biometrics</h3>

            <div class="portfolio_details_inner">
                <div class="row">
                    
                    <div class="offset-lg-1 col-lg-5">
                          <div class="portfolio_right_text mt-30">
                              <h5 class="text-uppercase">PhD Project, Manipal University Jaipur, India</h5>
                                   <p>Human gait recognition using the model-free approaches can be done through the analysis of moving shape and motion of the subject's body. The benefit of this approach is that the recognition can be performed at large distance with sufficiently low-resolution images. This approach is very simple and intuitive to extract gait signatures from the gait frames. </p>
                                    <ul class="list">
                                     <li><span>Duration </span>: Jan 2018 – Dec 2022</li>
                                     <li><span>Technology</span>: Deep Learning, Computer Vision, Biometrics (Gait)</li>
                                     <li><span>Application</span>: Surveillance</li>
								     <li><span>Interface</span>: GUI</li>
								     <li><span>Hardware</span>: Jetson Nano</li>
								     <li><span>Dataset</span>: CASIA, TUM, OUISIR</li>
								   
                                     </ul>
                             </div>
			                 </div>
							
							<div class="col-lg-6">
                            <div class="left_img">
                               <img class="img-fluid" src="img/project/4.png" alt="">
							   <div class="button-group-area mt-40"> <br>
								<a href="#" class="genric-btn link circle">Github Link</a>
								<a href="#" class="genric-btn link circle">Research paper Link</a>
								</div>
                            </div>
                        </div>
                 </div>
                


					<div class="row">
						<div class="col-md-3">
							<img src="img/project/1.png" alt="" class="img-fluid">
						</div>
						<div class="col-md-9 mt-sm-20 left-align-p">
							<p>The most interesting research in biometrics is automatic gait recognition when compared to other human unique features. Though there are many approaches to overcome the variations in gait recognition, still there are challenges to recognize a person. Challenges are distinctive gait datasets, degree of stability in identifying, sensing modality, covariates and spoofing effects, and exploring new algorithms. From the developmental perspective, human gait recognition has gained maturity in adopting recent methodologies to provide the high accuracy and the same was analyzed and studied in the view of vision based system. In this paper, the basic knowledge about the human gait is identified and explored. The comparative analysis with the existing techniques of gait recognition were discussed. In this work, the survey of recent deep architecture model on human gait identification, authentication and clinical applications were discussed.</p>
						</div>
					</div>
				
					<div class="row">
						<div class="col-md-9">
							<p class="text-right">The Gait Recognition System (GRS) is a biometric system that is utilized for security. Because
								the computational complexity of GRS is so great, a high-end hardware setup is required. It
								is tough to execute such algorithms on edge devices and if executed on cloud then network
								connectivity and data security issue arises. Furthermore, changes in an individual’s movement
								and wearing clothing, and carrying a bag are key covariate variables that affect a system’s
								performance. Furthermore, with GRS, identification under varied view angles is a significant
								difficulty. In this, a unique, completely automated, and optimal technique for edge devices for
								GRS under various covariates is proposed using deep learning as DeepGait.
								Preprocessing original video frames, creating an optimal deep learning pipeline utilizing the
								CNN model for feature extraction, reducing additional features, hyper tuning the network
								for all covariate circumstances, and ultimately subject detection are the key phrases. The
								extraction of CNN features is a crucial phase to obtain the most functional features. To do this,
								we carefully pick network settings. We created a network after running the datasets numerous
								times and attaining the lowest error rate. In the CASIA B dataset, eleven distinct view angles,
								occlusion in carrying state, and apparel variation are employed in the assessment method. An
								accuracy of 98.64 percent is reached on the jetson nano. Compared to current state-of-the-art
								methodologies, the results indicate a considerable increase in accuracy and runtime on edge
								devices like the Jetson Nano.</p>
							
						</div>
						<div class="col-md-3">
							<img src="img/project/2.png" alt="" class="img-fluid">
						</div>

					</div>
                        
						<div class="row">
							<div class="col-md-4">
								<div class="single-defination">
									<h4 class="mb-20">Application Interface</h4>
									<img src="img/project/3.png" alt="" class="img-fluid">
								</div>
							</div>
							<div class="col-md-4">
								<div class="single-defination">
									<h4 class="mb-20">Accuracy</h4>
									<img src="img/project/5.png" alt="" class="img-fluid">
								</div>
							</div>
							<div class="col-md-4">
								<div class="single-defination">
									<h4 class="mb-20">Results</h4>
									<img src="img/project/6.png" alt="" class="img-fluid">

								</div>
							</div>
						</div>
            </div>



			<div class="section-top-border">
				<br>
				<h3 style="color:MediumBlue "class="text-heading title_color">2. Face De-Identification Pipeline Based on Physiological & Machine Recognition Experiments Using Deep Learning</h3>
				<div class="portfolio_details_inner">
					<div class="row">
						<div class="offset-lg-1 col-lg-5">
							  <div class="portfolio_right_text mt-30">
								  <h5 class="text-uppercase">Croatian Government funded project with with Prof. Dr. Scientist. Slobodan Ribarić, Zagreb Croatia</h5>
									   <p>We proposed a reversible face de identification pipeline that modifies face geometry and texture. Fourteen parameters for geometrical modification are used. For texture modification fixed face texture template is used. </p>
										<ul class="list">
										 <li><span>Duration </span>: 5 Sept 2016 – 30 Dec 2018</li>
										 <li><span>Technology</span>: Deep Learning, Computer Vision, Biometrics (Face)</li>
										 <li><span>Application</span>: Surveillance, De-Identification</li>
										 <li><span>Hardware</span>: Jetson Nano</li>
										 <li><span>Dataset</span>: Famoud faces</li>
										 </ul>
								</div>
						  </div>
     							<div class="col-lg-6">
								<div class="left_img">
								   <img class="img-fluid" src="img/project/9.png" alt="">
								   <div class="button-group-area mt-40"> <br>
									<a href="#" class="genric-btn link circle">Github Link</a>
									<a href="#" class="genric-btn link circle">Research paper Link</a>
									</div>
								</div>
							</div>
					 </div>
					          <div class="row">
							      <div class="col-md-3"><video width="280" height="240" controls><source src="img/project/1.mp4" type="video/mp4"></video></div>
							      <div class="col-md-9 mt-sm-20 left-align-p">
							     	<p>We compiled a set of 30 face images of famous people (7 females and 23 males with ages ranging from 30 to 75) from politics, sports, business and entertainment. The used testing procedure is like the one used for diagnosing of the prosopagnosia a neurological disorder characterized by the inability to recognize human faces. Images of de-identified faces of famous persons are presented to the test subjects with a request to try to recognize them. Obtained answers are recorded for later matching with ground truth answers. The main aim of the performed testing is to evaluate the impact of geometrical and texture modifications on human ability to recognize faces. The evaluation is performed by means of crowdsourcing performed by 150 test subjects (20 females and 130 males). The test subjects were informed that faces in the tests are de-identified faces of famous people. The background (ie context) and biometrical ques like hair and ears, that a user can use for face identification are removed in all tests.</p>
							       </div>
						       </div>
				 </div>
							
							<div class="row">
								<div class="col-md-4">
									<div class="single-defination">
										<h4 class="mb-20">Results</h4>
										<p> We have investigated impact of various geometrical and texture modifications of face components like cyes, eyebrows, nose and lips on ability of humans and machines to recognize faces. The crowdsourcing and machine face recognition experiments were performed on images of famous people collected from the Internet. The obtained results in both types of experiments showed that face texture has stronger impact on a level of privacy protection then face geometry shape) modifications. </p>
									</div>
								</div>
								<div class="col-md-4">
									<div class="single-defination">
										<h4 class="mb-20">Application Interface</h4>
										<img src="img/project/7.jpeg" alt="" class="img-fluid">
									</div>
								</div>
								<div class="col-md-4">
									<div class="single-defination">
										<h4 class="mb-20">face Dataset</h4>
										<img src="img/project/8.png" alt="" class="img-fluid">
	
									</div>
								</div>
							</div>
				</div>
	


				









				<div class="section-top-border">
	<br>
					<h3 style="color:MediumBlue "class="text-heading title_color">3. Classification Of Gait Data Using Machine Learning Techniques To Categorise Human Locomotion</h3>
				
					<div class="portfolio_details_inner">
						<div class="row">
							
							<div class="offset-lg-1 col-lg-5">
								  <div class="portfolio_right_text mt-30">
									  <h5 class="text-uppercase">M.Tech Project, VCE Rohtak</h5>
										   <p>Shown the importance of gait recognition in order to detect whether a human GAIT is normal gait or pathological gait.  </p>
											<ul class="list">
											 <li><span>Duration </span>: 5 Sept 2014 – 30 June 2016

											 </li>
											 <li><span>Technology</span>: Machine Learning, Computer Vision

											 </li>
											 <li><span>Application</span>: Medical, Early detection</li>
											 <li><span>Hardware</span>: Jetson Nano</li>
											 <li><span>Dataset</span>: Crouch dataset. 
											 </li>
										   
											 </ul>
									 </div>
									 </div>
									
									<div class="col-lg-6">
									<div class="left_img">
									   <img class="img-fluid" src="img/project/10.png" alt="">
									   <div class="button-group-area mt-40"> <br>
										<a href="#" class="genric-btn link circle">Github Link</a>
										<a href="#" class="genric-btn link circle">Research paper Link</a>
										</div>
									</div>
								</div>
						 </div>
						
		
		
							<div class="row">
								<video width="280" height="240" controls>
									<source src="img/project/2.mp4" type="video/mp4">
									</video>
								<div class="col-md-9 mt-sm-20 left-align-p">
									<p>Three main approaches in gait classification i.e. back propagation and KNN. After comparing the testing and training results we get better results using back propagation learning technique. The whole work of paper is to describe the classification technique of different type of GAIT into following four categories: Normal, crouch2, crouch3, crouch4 using back propagation and KNN. When the training of data is done then the output of will be either four options i.e. Normal, crouch2, crouch3, crouch4. If the testing data set is tested on the proposed system then the output must be normal GAIT if the classifier classifies it as normal data or the output will be abnormal GAIT data if the classifier classifies it as crouch2, crouch3, crouch4 in any of these three categories.										.</p>
								</div>
							</div>
						
							
								
								<div class="row">
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Application Interface</h4>
											<img src="img/project/12.png" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Accuracy</h4>
											<img src="img/project/11.png" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Results</h4>
											<p>So we first select the feature and identify the principal feature then we classify gait data and use different machine learning techniques (K-mean, KNN and Back Propagation) and performance comparison is shown. Experimental results on real time datasets proposed method is better than previous method as far as humanoid locomotion classification is concerned.</p>
		
										</div>
									</div>
								</div>
					</div>
		
				</div>
	
	






					<div class="section-top-border">

					<h3 style="color:MediumBlue "class="text-heading title_color">4. Low-cost IoT enabled Board Marker using Image Processing</h3>

					<div class="portfolio_details_inner">
						<div class="row">
							
							<div class="offset-lg-1 col-lg-5">
								  <div class="portfolio_right_text mt-30">
									  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
										   <p>This project is based on robotics. Robots have been easing human tasks since decades and in addition to the numerous tasks they perform for us, this time its desired that they contribute towards modern education. </p>
											<ul class="list">
											 <li><span>Duration </span>: Jan 2019 – Dec 2020</li>
											 <li><span>Technology</span>: Image Processing, IoT</li>
											 <li><span>Application</span>: Automation, class room teaching

											 </li>
											 <li><span>Interface</span>: GUI - Processing Software version 2.2</li>
											 <li><span>Hardware</span>: Arduino mega, 9V DC supply, Two Stepper Motors, One Servo Motor, Male-Male, Male-Female and Female-Female Jumper Cables, 16 Teeth Pulley Cable, Drawing Board, Drawing Instrument (e.g. Marker), L293D Motor Shield, Heat Sinks, Weights</li>
											 <li><span>Dataset</span>: JPG/PNG and SVG format 

											 </li>
										   
											 </ul>
									 </div>
									 </div>
									
									<div class="col-lg-6">
									<div class="left_img">
									   <img class="img-fluid" src="img/project/13.jpeg" alt="">
									   <div class="button-group-area mt-40"> <br>
										<a href="#" class="genric-btn link circle">Github Link</a>
										<a href="#" class="genric-btn link circle">Research paper Link</a>
										</div>
									</div>
								</div>
						 </div>
						
		
		
							<div class="row">
								<video width="280" height="240" controls>
									<source src="img/project/3.mp4" type="video/mp4">
									</video>
								<div class="col-md-9 mt-sm-20 left-align-p">
									<p>The project would be implemented keeping the teaching industry in mind but could also find its applications in areas that require designing. The basic idea of this project is that a robot would be fed a reference drawing, mostly text-based input or a diagram, which it would then replicate on an output area of our choice for example a white board or a notebook. The benefits of such a robot would be that it would save time that would be required to produce a complex and detailed diagram on a white board for teaching purposes and quite obviously be helpful in graphic designing on materials of our choice if implemented and refined further. The system works on a pulley system where an end-effector is manipulated using the pulleys. The application running on the PC is fed with the image which the recognizes the edges of the image and then sends command numbers to the microcontroller. The microcontroller upon receiving the command numbers performs commands associated with the received command numbers. The microcontroller rotates the stepper motor as per the need of the image to be drawn, the servo motor attached at the pen-holder acts as a pen lifting mechanism similar to the action we perform while writing or drawing. The pulley system adds to the maneuverability of the system while cutting down on costs. The system works as per predicted and produces good quality drawings on the drawing pad.</p>
								</div>
							</div>
						
							
								
								<div class="row">
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Application Interface</h4>
											<img src="img/project/14.jpeg" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Accuracy</h4>
											<img src="img/project/15.png" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Results</h4>
											<p>With the 9V of DC supply, the stepper motors run smoothly with no jitters. The circuit doesn’t experience overheating at this power supply. The systems performance is heavily dependent on the power supply. The pen-lift actions are fluid and precise and the system responds quickly to the drawing commands somewhere between 0.5-1 second.</p>
		
										</div>
									</div>
								</div>
					</div>
		
		
				</div>
		
		
		
		







					<div class="section-top-border">

					<h3 style="color:MediumBlue "class="text-heading title_color">5. IoT Enabled Mechanical Chess based on Artificial Intelligence</h3>

					<div class="portfolio_details_inner">
						<div class="row">
							
							<div class="offset-lg-1 col-lg-5">
								  <div class="portfolio_right_text mt-30">
									  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
										   <p>Automated AI chess board is a physical chess board which lets the player play against an AI the chess board is powered by raspberry Pi. </p>
											<ul class="list">
											 <li><span>Duration </span>: Jan 2018 – Dec 2019</li>
											 <li><span>Technology</span>: Artificial Intelligence, Robotics, IoT</li>
											 <li><span>Application</span>: Playing chess on a real chess board against machine
											 </li>
											 <li><span>Interface</span>: Raspberry Pi, L298n Motor drivers </li>
											 <li><span>Hardware</span>: Flexible Shaft Coupling, Stainless Steel Lead Screw Rod with Nut, Pillow Block Bearing, box/wood/table, servo motor, raspberry pi, arduino, Chess board, Magnets, Electromagnets, Spools, Belts</li>
											 <li><span>Dataset</span>: Stockfish 

											 </li>
										   
											 </ul>
									 </div>
									 </div>
									
									<div class="col-lg-6">
									<div class="left_img">
									   <img class="img-fluid" src="img/project/16.jpeg" alt="">
									   <div class="button-group-area mt-40"> <br>
										<a href="#" class="genric-btn link circle">Github Link</a>
										<a href="#" class="genric-btn link circle">Research paper Link</a>
										</div>
									</div>
								</div>
						 </div>
						
		
		
							<div class="row">
								<video width="280" height="240" controls>
									<source src="img/project/4.mp4" type="video/mp4">
									</video>
								<div class="col-md-9 mt-sm-20 left-align-p">
									<p>The Field of Artificial Intelligence is very vast but the prime goal of researchers and engineers is to enable the computer to think like a human. And a game of chess require a lot of thinking and logic, for a long time we have been playing the game of chess to exercise our brains and entertain ourselves at the same time. And we all love the feel of an original chess board. It would be really nice to play the game of chess against an Physical Board.

										Using the core XY motion of two motors we are enabling an electromagnet to span a whole chess board and the electromagnet will be responsible to slide the pieces across the whole board. Using two Nema 17 motors and two lead screw mechanism for our X and Y axises both powered by L2N8 motor drivers and a relay module to control the electromagnet the chess pieces with sufficiently strong magnets attached to the bottom will slide across the board. Using the Min-Max spanning tree providing an initial score to each player the goal of our AI is to minimize the score of the opposing player and trying to maintain its own score.
										
										User will make a move and depending on the move made by user our AI will generate a move tree containing possible moves. Every piece of the board has a rank and a score which is equal to the sum of all the ranks of alive pieces of that particular color will be assigned at each step to the color. Each node of the move tree contains the score corresponding to the move, treating scores as weights of the tree we apply Min-Max search to make the best possible move.</p>
								</div>
							</div>
						
							
								
								<div class="row">
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Application Interface</h4>
											<img src="img/project/17.png" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Results</h4>
											<img src="img/project/18.png" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Results</h4>
											<p>Tests were performed with certain parameters in mind Speed of AI: The AI performed above expectations in terms of speed. X motion track and motor: It ran fast and delivered accurately but made a lot of noise, indicating that it was not perfectly smooth. Y motion track and motor: It was smoother than the X track but was slightly slower due to the extra weight and added friction from the optical axis shafts. Electromagnet: It was stronger than expected and due to unevenness of board occasionally lost pieces, this was corrected by reducing the height of the mount on which it rested and increasing the voltage to give more power. We used a new approach to core XY motion which efficiently handles large weight while still maintaining the speed. The AI developed for our project is fast and very reliable for low end embedded systems.</p>
		
										</div>
									</div>
								</div>
					</div>
		
		
				</div>
		
		
		
		







					<div class="section-top-border">

					<h3 style="color:MediumBlue "class="text-heading title_color">6. Optimized Navigation using Deep Learning Technique for Automatic Guided Vehicle</h3>

					<div class="portfolio_details_inner">
						<div class="row">
							
							<div class="offset-lg-1 col-lg-5">
								  <div class="portfolio_right_text mt-30">
									  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
										   <p>This study tackled the mentioned problems with a straightforward and cost-effective solution, using end to end learning and replacing the numerous sensors with a camera and commandeering just the forward, backward, left, and right controls.</p>
											<ul class="list">
											 <li><span>Duration </span>: June 2018 – Dec 2019</li>
											 <li><span>Technology</span>: Deep Learning, IoT, Computer Vision</li>
											 <li><span>Application</span>: Smart car, Autonomous, Detect Stop Signs and Traffic Lights and Avoid Front Collision.



											 </li>
											 <li><span>Software</span>: Raspbian OS, 2. Python, OpenCV Libraries</li>
											 <li><span>Hardware</span>: RC Car, Raspberry Pi,  Ultrasonic sensor, PI Camera </li>
											 <li><span>Dataset</span>: Stimulator for autonomous car by Udacity generated training data



											 </li>
										   
											 </ul>
									 </div>
									 </div>
									
									<div class="col-lg-6">
									<div class="left_img">
									   <img class="img-fluid" src="img/project/19.png" alt="">
									   <div class="button-group-area mt-40"> <br>
										<a href="#" class="genric-btn link circle">Github Link</a>
										<a href="#" class="genric-btn link circle">Research paper Link</a>
										</div>
									</div>
								</div>
						 </div>
						
		
		
							<div class="row">
								<video width="280" height="240" controls>
									<source src="img/project/5.mp4" type="video/mp4">
									</video>
								<div class="col-md-9 mt-sm-20 left-align-p">
									<p>Autonomous driving has passed the point of being called the biggest step, as the smart car revolution is already taking shape around the world. Self-driving cars are relevant if not prevalent and the biggest obstacles to reach the mass adoption are customer acceptance, cost, infrastructure and the reliance on several onerous algorithms that include perception, lane marking detection, path planning and variation in pathways. In this research, authors have used most popular method of deep learning i.e. Convolution Neural Network (CNN) to train the collected data on the VGG16 model. Later these have optimized directly by the proposed system with cropping each unnecessary image and mapping pixels from a single front-facing camera to direct steering instructions. It has been observed from the experimental work that proposed model has given better result than the existing work i.e. increase in the accuracy from 88% (Udacity training dataset) to 98% (proposed). This model is suitable for industrial use and robust in real time scenarios therefore can be applied in modern industrialized systems.</p>
								</div>
							</div>
						
							
								
								<div class="row">
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Application Interface</h4>
											<img src="img/project/20.jpeg" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Results</h4>
											<img src="img/project/21.png" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Accuracy</h4>
											<p>After rigorous test runs on tracks apart from which the car is trained on, it is concluded that the car can function competently in a controlled environment. The neural network is also working fittingly and gave 98% results.</p>
		
										</div>
									</div>
								</div>
					</div>
		
		
				</div>
		
		
		





					<div class="section-top-border">

						<h3 style="color:MediumBlue "class="text-heading title_color">7. IoT based Smart Assistance Spoon for Parkinson Patients</h3>
	
						<div class="portfolio_details_inner">
							<div class="row">
								
								<div class="offset-lg-1 col-lg-5">
									  <div class="portfolio_right_text mt-30">
										  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
											   <p>Our primary objective is to make the Robotic arm, having 4 servo motors to interface with the development of a microcontroller based Robotic arm.</p>
												<ul class="list">
												 <li><span>Duration </span>: Jan 2017 – Dec 2018</li>
												 <li><span>Technology</span>: Artificial Intelligence, Robotics, IoT</li>
												 <li><span>Application</span>: Feed the patient with no efforts needed

												 </li>
												 <li><span>Interface</span>: Android App</li>
												 <li><span>Hardware</span>: Raspberry Pi, Servo Motors, Stepper motor, Robotic arm, Breadboard, Jumper Cables, Other modules like Bluetooth module, WIFI module </li>
												 <li><span>Dataset</span>:  Patient height


	
	
	
												 </li>
											   
												 </ul>
										 </div>
										 </div>
										
										<div class="col-lg-6">
										<div class="left_img">
										   <img class="img-fluid" src="img/project/22.png" alt="">
										   <div class="button-group-area mt-40"> <br>
											<a href="#" class="genric-btn link circle">Github Link</a>
											<a href="#" class="genric-btn link circle">Research paper Link</a>
											</div>
										</div>
									</div>
							 </div>
							
			
			
								<div class="row">
									<video width="280" height="240" controls>
										<source src="img/project/6.mp4" type="video/mp4">
										</video>
									<div class="col-md-9 mt-sm-20 left-align-p">
										<p>The arm control by robotics is very popular in the world of robotics. The essential part of the robotic arm is a programmable micro controller-based brick capable of driving basically four servos to form an anthropomorphic structure.

											Our primary objective is to make the Robotic arm, having 4 servo motors to interface with the development of a microcontroller based Robotic arm. It provides more interfaces to the outside world and has larger memory to store many programs.
											
											The technology for assisting people who are functionally challenged has improved over the recent decades. A group that suffer from this ailment are people with Parkinson’s disease. Parkinson’s disease (PD) is a neurological degenerative disease that causes uncontrollable shaking and makes it difficult for the affected person to eat. There is no cure for PD, but there is technology and potential for new technology that can help people who carry the disease with their daily lives.
											
											The machine will be of great use to perform repetitive tasks of picking and placing of small edibles upto 500 grams in one serving.
											
											It can be used to do small assembly work effectively due to its great added accuracy for placement purpose, which has further extended scope of our project.</p>
									</div>
								</div>
							
								
									
									<div class="row">
										<div class="col-md-4">
											<div class="single-defination">
												<h4 class="mb-20">Application Interface</h4>
												<img src="img/project/23.jpeg" alt="" class="img-fluid">
											</div>
										</div>
										<div class="col-md-4">
											<div class="single-defination">
												<h4 class="mb-20">Results</h4>
												<img src="img/project/24.jpeg" alt="" class="img-fluid">
											</div>
										</div>
										<div class="col-md-4">
											<div class="single-defination">
												<h4 class="mb-20">Accuracy</h4>
												<p>The robotic arm model was successfully implemented which was able to pick up food from the tray and direct it towards the mouth of the user. The Bluetooth app for connecting to the arm, controlling and terminating the process was also developed.</p>
			
											</div>
										</div>
									</div>
						</div>
			
					</div>
					











				
			<div class="section-top-border">
				<br>
				<h3 style="color:MediumBlue "class="text-heading title_color">8. Unmanned Aerial Vehicle for Surveillance</h3>
				<div class="portfolio_details_inner">
					<div class="row">
						<div class="offset-lg-1 col-lg-5">
							  <div class="portfolio_right_text mt-30">
								  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
									   <p>Drone is an all-purpose medical toolkit that can be automatically flown to any emergency situation and used to guide citizens to make non-technical lifesaving procedures.</p>
										<ul class="list">
										 <li><span>Duration </span>: Jan 2020 – Present</li>
										 <li><span>Technology</span>: Artificial Intelligence, Robotics, IoT</li>
										 <li><span>Application</span>: Surveillance</li>
										 <li><span>Interface</span>: Controlling quadcopter with mobile application</li>
										 <li><span>Hardware</span>: Quadcopter, Battery, Ardupilot, ESCs(electronic speed controllers), Gyroscope, wifi module, Installing camera with quadcopter, Inserting microphone in drone, Interfacing different Sensors</li>
										 <li><span>Dataset</span>: Environment

										 </li>
										 </ul>
								</div>
						  </div>
     							<div class="col-lg-6">
								<div class="left_img">
								   <img class="img-fluid" src="img/project/23.png" alt="">
								   <div class="button-group-area mt-40"> <br>
									<a href="#" class="genric-btn link circle">Github Link</a>
									<a href="#" class="genric-btn link circle">Research paper Link</a>
									</div>
								</div>
							</div>
					 </div>
					          <div class="row">
							      <div class="col-md-3"><video width="280" height="240" controls><source src="img/project/7.mp4" type="video/mp4"></video></div>
							      <div class="col-md-9 mt-sm-20 left-align-p">
							     	<p>When a medical emergency takes place in natural flood, the response time can make all the difference between a life saved and a life lost. Unfortunately, ambulances can’t reach in the place due to over water flooding, in which time a cardiac arrest victim may have already succumbed to a lack of oxygen to the brain. Drone is an all-purpose medical toolkit that can be automatically flown to any emergency situation and used to guide citizens to make non-technical lifesaving procedures.

										With our drone we dramatically increase this survival rate. The incorporation of a two-way, video supported, communication channel in the drone between operators and the first responders will improve first care. Successful drone usage by lay-persons is currently at 20%. With personalized instructions and communication on the drone, this can be increased to 90%. In short, the drone helps to save lives by extending existing emergency infrastructure with a network of fast and compressed video broadcasting channel capable of bringing emergency supplies and establishing communication between dedicated source and destination.</p>
							       </div>
						       </div>
				 </div>
							
							<div class="row">
								<div class="col-md-4">
									<div class="single-defination">
										<h4 class="mb-20">Results</h4>
										<p>In this project, we are building a drone which can provide live video coverage for monitoring purpose. Additionally, GPS module can also be used. GPS module will give us the location of the drone. Then, after attaching telemetry device, waypoints can be given to drone so that it can automatically go to the required location and come back to the same place from where it was launched.</p>
									</div>
								</div>
								<div class="col-md-4">
									<div class="single-defination">
										<h4 class="mb-20">Application Interface</h4>
										<img src="img/project/26.jfif" alt="" class="img-fluid">
									</div>
								</div>
								<div class="col-md-4">
									<div class="single-defination">
										<h4 class="mb-20">Hardware Interface</h4>
										<img src="img/project/25.jpg" alt="" class="img-fluid">
	
									</div>
								</div>
							</div>
				</div>
		











				
				<div class="section-top-border">
					<br>
					<h3 style="color:MediumBlue "class="text-heading title_color">9. Smartphone-based Sleep Staging using 1-Channel EEG</h3>
					<div class="portfolio_details_inner">
						<div class="row">
							<div class="offset-lg-1 col-lg-5">
								  <div class="portfolio_right_text mt-30">
									  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
										   <p>Our project allows for accurate sleep staging, informing the user with information such as the duration of each sleep stage he was in, the current sleep stage, etc with the convenience of it being on a smartphone without the need for any external servers, thus allowing for sleep staging without the need of network access. </p>
											<ul class="list">
											 <li><span>Duration </span>: Jan 2021 – Present</li>
											 <li><span>Technology</span>: Artificial Intelligence, Deep Learning, IoT</li>
											 <li><span>Application</span>: Detect sleep stages

											 </li>
											 <li><span>Interface</span>: Smartphone App</li>
											 <li><span>Hardware</span>: EEG headset</li>
											 <li><span>Dataset</span>: Physionet.org which consisted of different signals from the EEG


	
											 </li>
											 </ul>
									</div>
							  </div>
									 <div class="col-lg-6">
									<div class="left_img">
									   <img class="img-fluid" src="img/project/28.jpeg" alt="">
									   <div class="button-group-area mt-40"> <br>
										<a href="#" class="genric-btn link circle">Github Link</a>
										<a href="#" class="genric-btn link circle">Research paper Link</a>
										</div>
									</div>
								</div>
						 </div>
								  <div class="row">
									  <div class="col-md-3"><video width="280" height="220" controls><source src="img/project/8.mp4" type="video/mp4"></video></div>
									  <div class="col-md-8 mt-sm-15 left-align-p">
										 <p>Our project is a smartphone-based sleep staging system with two main components: the application and the EEG band. The EEG band can be connected to the smartphone using Bluetooth, which sends the EEG signals to the phone for analysis of the sleeping user. The application then scores the sleep presenting the information to the user to infer their sleeping patterns. It can also be used for development of user interface that triggers stimuli in specific sleep stages based on further improvements.

The project employs the use of a Machine Learning model paired with a smartphone to analyse the EEG input, allowing for classification of the sleep stage of a person. We will be using an EEG with wireless functionality. 

The project has been divided into 4 phases: Research, ML model development, Android app development and Testing/Improvements.</p>
									   </div>
								   </div>
					 </div>
								
								<div class="row">
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Results</h4>
											<p>We have successfully made a ML model and an Android app demonstrating the technology. Since there was a lack of an EEG hardware, the app is using sample data. The app successfully demonstrated the input, which can use any sample data that the user gives and can show the result in the form of a chart as well as show the sleep stage. The project is a success in its circumstances. With newer technology and hardware there are many improvements to be made.</p>
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Hardware Interface</h4>
											<img src="img/project/29.jpeg" alt="" class="img-fluid">
										</div>
									</div>
									<div class="col-md-4">
										<div class="single-defination">
											<h4 class="mb-20">Accuracy</h4>
											<img src="img/project/27.png" alt="" class="img-fluid">
		
										</div>
									</div>
								</div>
					</div>
			





				
					<div class="section-top-border">
						<br>
						<h3 style="color:MediumBlue "class="text-heading title_color">10. IoT based Cloud Enabled Automated Weather Reporting and Prediction System</h3>
						<div class="portfolio_details_inner">
							<div class="row">
								<div class="offset-lg-1 col-lg-5">
									  <div class="portfolio_right_text mt-30">
										  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
											   <p>The objective is to monitor and report weather conditions so that one is informed beforehand and necessary actions can be taken to reduce the damage by any calamity by forecasting it.</p>
												<ul class="list">
												 <li><span>Duration </span>: Jan 2018 – Dec 2019</li>
												 <li><span>Technology</span>: Machine Learning, IoT, Data Analytics, Cloud Computing</li>
												</li>
												 <li><span>Application</span>: Weather Reporting and Prediction </li>
												 <li><span>Interface</span>: Arduino IDE, Website, Thingspeak, MATLAB, Jupyter Lab</li>
												 <li><span>Hardware</span>: Wi-Fi module, Arduino Mega, LCD Display, MQ-135, BMP-180, Dust Sensor, Rain Sensor, LDR Sensor, MQ-7, DHT11</li>
												 <li><span>Dataset</span>: Kaggle


		
												 </li>
												 </ul>
										</div>
								  </div>
										 <div class="col-lg-6">
										<div class="left_img">
										   <img class="img-fluid" src="img/project/30.jpeg" alt="">
										   <div class="button-group-area mt-40"> <br>
											<a href="#" class="genric-btn link circle">Github Link</a>
											<a href="#" class="genric-btn link circle">Research paper Link</a>
											</div>
										</div>
									</div>
							 </div>
									  <div class="row">
										  <div class="col-md-3"><video width="280" height="240" controls><source src="img/project/9.mp4" type="video/mp4"></video></div>
										  <div class="col-md-9 mt-sm-20 left-align-p">
											 <p>The weather can have great impact on lives. Weather changes can influence wide range of human activities and affect agriculture and transportation. The project’s objective is to monitor and report weather conditions so that one is informed beforehand and necessary actions can be taken to reduce the damage by any calamity by forecasting it. Through weather monitoring system we can collect the information according to current and previous data we can produce the results in graphical manner. In addition to this, there is one research paper that has discussed monitoring these three environmental conditions; however, there has been no mention about having actuators to modify. So, our main idea was to coin a system that can sense the main components that formulates the weather and can be able to forecast the weather without human error. We will be analyzing temperature, pollutants, humidity and pressure and will predict the weather. Existing models are expensive in contrast to ours and hence it will make monitoring local area feasible as it will be cheaper.  </p>
										   </div>
									   </div>
						 </div>
									
									<div class="row">
										<div class="col-md-4">
											<div class="single-defination">
												<h4 class="mb-20">Results</h4>
												<p>The parameter values collected from DHT11 sensor are stored on internet using cloud, which is used for further analysis using MATLAB. The data collected through sensors can be viewed on a web portal.</p>
											</div>
										</div>
										<div class="col-md-4">
											<div class="single-defination">
												<h4 class="mb-20">Application Interface</h4>
												<img src="img/project/31.jpeg" alt="" class="img-fluid">
											</div>
										</div>
										<div class="col-md-4">
											<div class="single-defination">
												<h4 class="mb-20">Accuracy</h4>
												<img src="img/project/32.png" alt="" class="img-fluid">
			
											</div>
										</div>
									</div>
						</div>
				
	












				
						<div class="section-top-border">
							<br>
							<h3 style="color:MediumBlue "class="text-heading title_color">11. IoT-Based Cloud-Enabled Smart Electricity Management System</h3>
							<div class="portfolio_details_inner">
								<div class="row">
									<div class="offset-lg-1 col-lg-5">
										  <div class="portfolio_right_text mt-30">
											  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
												   <p>In this project, we implemented the basic functionalities of an electric meter in a smarter and more efficient manner. We made an attempt to simplify the process of electric meter reading collection and analysis, thereby making it efficient instead of the tedious traditional approach.</p>
													<ul class="list">
													 <li><span>Duration </span>: 12 October 2016 - December 2017</li>
													 <li><span>Technology</span>: IoT, Cloud Computing</li>
													 <li><span>Application</span>: Energy saving, Meter billing


		
													 </li>
													 <li><span>Interface</span>: Smartphone App</li>
													 <li><span>Hardware</span>: Wi-Fi module, Arduino Uno, LCD Display, Step-down, Transformer, rectifier, energy meter, current sensor, bulb, resistor</li>
													
			
													 </li>
													 </ul>
													 <div class="button-group-area mt-40"> <br>
														<a href="#" class="genric-btn link circle">Github Link</a>
														<a href="#" class="genric-btn link circle">Research paper Link</a>
														</div>
											</div>
									  </div>
											 <div class="col-lg-6">
											<div class="left_img">
											   <img class="img-fluid" src="img/project/33.jpeg" alt="">
											
											</div>
										</div>
								 </div>
										  <div class="row">
											  <div class="col-md-3"><video width="280" height="220" controls><source src="img/project/10.mp4" type="video/mp4"></video></div>
											  <div class="col-md-9 mt-sm-20 left-align-p">
												 <p>In the age of digitalization, Internet-based applications are gaining popularity at an exponential rate. Today, everyone wants to make their lives easier and devices smarter. In the age of automation, most of the devices we interact with on a day-to-day basis, for example, air conditioners, refrigerators, etc. are made increasingly intelligent to simplify our lives and make it comfortable. Using the principles of IoT and AI, we can create home automation devices such as automatic security devices and e-meters that make our homes smarter and more secure. Keeping a track of how much electricity is consumed per household becomes imperative seeing the rate at which global warming is increasing. Gone are the days where users had to go to meter reading room and take down readings. By employing IoT concepts, we can simplify this tedious process and record the reading over cloud for easy accessibility. The major advantage of digitalizing the process is that the user has the facility to view his consumption remotely, i.e., anywhere in the world. This also enables the user to keep a log of how many units of electricity a device is consuming and with how much amount the user is being charged fairly or not.

													 Also, it was observed that the new system was  more accurate and faster than the existing systems. By simple application of IoT principles, current sensor, and microcontrollers, we have streamlined the process and moved the process to a cloud-based application. This not only provides remote accessibility but also accountability and reliability.</p>
											   </div>
										   </div>
							 </div>
										
										<div class="row">
											<div class="col-md-4">
												<div class="single-defination">
													<h4 class="mb-20">Results</h4>
													<p>We have successfully made a ML model and an Android app demonstrating the technology. Since there was a lack of an EEG hardware, the app is using sample data. The app successfully demonstrated the input, which can use any sample data that the user gives and can show the result in the form of a chart as well as show the sleep stage. The project is a success in its circumstances. With newer technology and hardware there are many improvements to be made.</p>
												</div>
											</div>
											<div class="col-md-4">
												<div class="single-defination">
													<h4 class="mb-20">Hardware Interface</h4>
													<img src="img/project/34.jpeg" alt="" class="img-fluid">
												</div>
											</div>
											<div class="col-md-4">
												<div class="single-defination">
													<h4 class="mb-20">Accuracy</h4>
													<img src="img/project/35.png" alt="" class="img-fluid">
				
												</div>
											</div>
										</div>
							</div>
					
		
		



				
	












				
							<div class="section-top-border">
								<br>
								<h3 style="color:MediumBlue "class="text-heading title_color">12. Human Assistant Robot</h3>
								<div class="portfolio_details_inner">
									<div class="row">
										<div class="offset-lg-1 col-lg-5">
											  <div class="portfolio_right_text mt-30">
												  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
													   <p>Making Self-Navigation Robot to navigate autonomously in any environment. When designing human robot collaboration system or Human assistant robot the first thing to flash in our mind is the objective since there are numerous tasks to be carried out in human bio daily life by a human being or what should be the productive throughput of such collaborations.</p>
														<ul class="list">
														 <li><span>Duration </span>: June to December 2018</li>
														 <li><span>Technology</span>: Robotics, IoT, Artificial Intelligence</li>
														 <li><span>Application</span>: Human Assistance</li>
														 <li><span>Interface</span>: Smartphone App</li>
														 <li><span>Hardware</span>: Arduino Mega, Raspberry pi, Step-down, Transformer, resistor, Servo-motor, IR sensor, UV sensor, Breadboard, Wheels</li>
														
				
														 </li>
														 </ul>
														 <div class="button-group-area mt-40"> <br>
															<a href="#" class="genric-btn link circle">Github Link</a>
															<a href="#" class="genric-btn link circle">Research paper Link</a>
															</div>
												</div>
										  </div>
												 <div class="col-lg-6">
												<div class="left_img">
												   <img class="img-fluid" src="img/project/36.jpeg" alt="">
												
												</div>
											</div>
									 </div>
											  <div class="row">
												  <div class="col-md-3"><video width="280" height="220" controls><source src="img/project/11.mp4" type="video/mp4"></video></div>
												  <div class="col-md-9 mt-sm-20 left-align-p">
													 <p>In the age of digitalization, Internet-based applications are gaining popularity at an exponential rate. Today, everyone wants to make their lives easier and devices smarter. In the age of automation, most of the devices we interact with on a day-to-day basis, for example, air conditioners, refrigerators, etc. are made increasingly intelligent to simplify our lives and make it comfortable. Using the principles of IoT and AI, we can create home automation devices such as automatic security devices and e-meters that make our homes smarter and more secure. Keeping a track of how much electricity is consumed per household becomes imperative seeing the rate at which global warming is increasing. Gone are the days where users had to go to meter reading room and take down readings. By employing IoT concepts, we can simplify this tedious process and record the reading over cloud for easy accessibility.
	
														</p>
												   </div>
											   </div>

											   <div class="row">
												<div class="col-md-7">
													<p class="text-right"> The major advantage of digitalizing the process is that the user has the facility to view his consumption remotely, i.e., anywhere in the world. This also enables the user to keep a log of how many units of electricity a device is consuming and with how much amount the user is being charged fairly or not. Also, it was observed that the new system was  more accurate and faster than the existing systems. By simple application of IoT principles, current sensor, and microcontrollers, we have streamlined the process and moved the process to a cloud-based application. This not only provides remote accessibility but also accountability and reliability. This research work stirs plans to develop a personal assistant robot that has following features. The robot can perceive normal objects that are seen in day-to-day lives and can pass on the information as voice messages which can significantly help old and outwardly impeded individuals. It can read out text messages from an image which again can significantly help old and outwardly impeded individuals. It acts as an empathy robot by perceiving facial feelings and is programmed to play some music according to the detected emotion of the user. It also can find out whether the user is wearing a mask and if not can give an alert voice message. Lastly, it can also has Amazon Alexa speech assistant technology.
												</div>
												<div class="col-md-4">
													<img src="img/project/38.jpeg" alt="" class="img-fluid">
												</div>
						




								 </div>
											
											<div class="row">
												<div class="col-md-4">
													<div class="single-defination">
														<h4 class="mb-20">Results</h4>
														<p>We have successfully made and navigated Human Assistant Robot. The HAR (Human Assistant Robot) is a personal assistant robot designed to take over the task of retrieving water bottles from a fixed location inside office such as refrigerator of shelf, making tea or coffee without the need for human intervention in basic pick and place functions. The design goals for this project are to create applications working in concert to complete the task of picking of bottles (or any other goods).</p>
													</div>
												</div>
												<div class="col-md-4">
													<div class="single-defination">
														<h4 class="mb-20">Interface</h4>
														<img src="img/project/37.png" alt="" class="img-fluid">
													</div>
												</div>
												<div class="col-md-4">
													<div class="single-defination">
														<h4 class="mb-20">Results</h4>
														<div class="col-md-3"><video width="280" height="220" controls><source src="img/project/12.mp4" type="video/mp4"></video></div>					
													</div>
												</div>
											</div>
								</div>
						
			
			
	
	
	
	










				
								<div class="section-top-border">
									<br>
									<h3 style="color:MediumBlue "class="text-heading title_color">13. Drone Based Flying Solution for Medical Emergencies intended in Disaster Management </h3>
									<div class="portfolio_details_inner">
										<div class="row">
											<div class="offset-lg-1 col-lg-5">
												  <div class="portfolio_right_text mt-30">
													  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India </h5>
														   <p>Drone will fly back to doctor and necessary medications are placed on the Drone and this time Drone is used to carry the medications to patient for temporary relief or extension of emerging situation for certain amount of time.</p>
															<ul class="list">
															 <li><span>Duration </span>: July 2016 Dec 2016</li>
															 <li><span>Technology</span>: Robotics, IoT, Artificial Intelligence</li>
															 <li><span>Application</span>: Disaster management, Emergency management
	 </li>
															 <li><span>Interface</span>: IBM Bluemix cloud, Netbeans IDE, Android studio, Windows 10 OS, Putty remote server</li>
															 <li><span>Hardware</span>: Raspberry Pi 3 model B, Compact Drone capable of carrying 2kgs payload into it, Blood pressure sensor, BMP180 Biometric temperature and pressure sensor, ECG sensor, FSR (Force sensitive resister), Pulse sensor, 7V to 12V DC power supply, Android device, USB cables, Raspberry pi camera, Connecting wires, GSM adapter, GPS and GPRS modules, Electronic components like resisters, capacitors, operational amplifiers.</li>
															
					
															 </li>
															 </ul>
															
													</div>
											  </div>
													 <div class="col-lg-6">
													<div class="left_img">
													   <img class="img-fluid" src="img/project/39.png" alt="">
													
													</div>
													<div class="button-group-area mt-40"> <br>
														<a href="#" class="genric-btn link circle">Github Link</a>
														<a href="#" class="genric-btn link circle">Research paper Link</a>
														</div>
												</div>
										 </div>
												  <div class="row">
													  <div class="col-md-3"><video width="280" height="220" controls><source src="img/project/100.mp4" type="video/mp4"></video></div>
													  <div class="col-md-9 mt-sm-20 left-align-p">
														 <p>The proposed research plan we are using sensors for monitoring vital health parameters such as, ECG, heartbeat, Temperature, and Pressure. And making a wearable device which includes all sensors into it and a Drone is used to carry this device from one place to free defined destination place. Basically we are using that wearable device to measure bio parameters and put that data into the cloud using various available internet protocols, and that data is made accessible to a doctor or medical practitioner for making preliminary medical decisions. Data is continuously gathered to an automatic processing system and made available to medical staff, who may take necessary actions in emergency cases. Doctor will have an application to fetching the data from cloud to his phone in real-time. Based on the data variations from patient’s body and with his medical experience he is most likely to take appropriate prescriptions and suggest these to other end for treatment purpose. Mean time while doctor analyzing the data from cloud, the Drone will fly back to doctor and necessary medications are placed on the Drone and this time Drone is used to carry the medications to patient for temporary relief or extension of emerging situation for certain amount of time.</p>
													   </div>
												   </div>
									 </div>
												
												<div class="row">
													<div class="col-md-4">
														<div class="single-defination">
															<h4 class="mb-20">Results</h4>
															<p>Designed a sensor protocol hardware wearable architecture, which can accommodate a variety of sensors for measuring bio parameters, algorithms, android applications. Developed energy-efficient and performance-guaranteed cooperative application. Implemented and assessed the performance of the protocol stack in a WBAN tested. Integrated the proposed solutions in real medical devices and validate them in real working environments. Data which is stored in cloud can be further enhanced for predictive maintenance of health in cloud health service system.</p>
														</div>
													</div>
													<div class="col-md-4">
														<div class="single-defination">
															<h4 class="mb-20">Interface</h4>
															<img src="img/project/40.jpg" alt="" class="img-fluid">
														</div>
													</div>
													<div class="col-md-4">
														<div class="single-defination">
															<h4 class="mb-20">Hardware Interface</h4>
															<img src="img/project/41.jpeg" alt="" class="img-fluid">
						
														</div>
													</div>
												</div>
									</div>
							
				
				
		
		
		
						






				
									<div class="section-top-border">
										<br>
										<h3 style="color:MediumBlue "class="text-heading title_color">14. Evaluating and Improving Chatbot Techniques</h3>
										<div class="portfolio_details_inner">
											<div class="row">
												<div class="offset-lg-1 col-lg-5">
													  <div class="portfolio_right_text mt-30">
														  <h5 class="text-uppercase">NLP Project, Manipal University Jaipur, India </h5>
															   <p>Development of a chatbot or an intelligent conversational agent using Machine learning/Deep learning techniques is an interesting sub-domain of NLP (Natural Language processing).</p>
																<ul class="list">
																 <li><span>Duration </span>: 20 May - 15 July 2017																 </li>
																 <li><span>Technology</span>: Deep Learning </li>
																 <li><span>Application</span>: Automation, Machine Assistant</li>
																 <li><span>Interface</span>: GUI</li>
																 <li><span>Dataset</span>: Reddit</li>
																
						
																 </li>
																 </ul>
																 <div class="button-group-area mt-40"> <br>
																	<a href="#" class="genric-btn link circle">Github Link</a>
																	<a href="#" class="genric-btn link circle">Research paper Link</a>
																	</div>
														</div>
												  </div>
														 <div class="col-lg-6">
														<div class="left_img">
														   <img class="img-fluid" src="img/project/44.png" alt="">
														
														</div>
														
													</div>
											 </div>
													  <div class="row">
														  <div class="col-md-3"><video width="280" height="220" controls><source src="img/project/100.mp4" type="video/mp4"></video></div>
														  <div class="col-md-9 mt-sm-20 left-align-p">
															 <p>Extensive work has been done in this field in the past decade. The functioning of these agents is limited. They are mostly retrieval-based agents and are also not aimed at holding conversations which emulate real human interaction. Among current chatbots, many are developed using rule-based techniques, simple machine learning algorithms or retrieval-based techniques which do not generate good results. In this project, I have experimented with the two main self-learning bot techniques, namely retrieval-based technique and generative technique. For easier demonstration, I have also developed a Graphical Web user interface, that allows us to explore both of these techniques. Chatbots today use datasets restricted to a particular domain. These chatbots cannot hold long conversations. They do not perform well if a generalized question out of their training context is asked. I have used the Reddit comments dataset. It contains comments, not just from one field, but from several fields and can better replicate human interactions. But this chatbot will require a filtering method to display only those answers which are conservative and professional. A scoring mechanism for been developed for this purpose. This paper also presents a new way to listen for audio input commands. Every voice based chatbot has a unique identifier, for example in case of Apple’s Siri, it is “Hey Siri!”. This command needs to be invoked in the beginning of the sentence in order for the chatbot to listen to the rest of the question. This paper presents a new method which will allow a user to interact with the chatbot in a more human way.</p>
														   </div>
													   </div>
										 </div>
													
													<div class="row">
														<div class="col-md-4">
															<div class="single-defination">
																<h4 class="mb-20">Results</h4>
																<p>The most widely used metric for evaluating such dialogue systems is BLEU [8], a metric measuring word overlaps originally developed for machine translation. BLEU analyses the co-occurrences of n-grams in the reference and the proposed responses. It computes the n-gram precision for the whole dataset, which is then multiplied by a brevity penalty to penalize short translations. For BLEU-N, N denotes the largest value of programs considered (usually N = 4). The project yielded a bleu score of 3.3 which is good enough for a generative chatbot with uncleaned data. The training loss decreased substantially after the model was trained on about 70k comments.</p>
															</div>
														</div>
														<div class="col-md-4">
															<div class="single-defination">
																<h4 class="mb-20">Interface</h4>
																<img src="img/project/42.png" alt="" class="img-fluid">
															</div>
														</div>
														<div class="col-md-4">
															<div class="single-defination">
																<h4 class="mb-20">Hardware Interface</h4>
																<img src="img/project/43.png" alt="" class="img-fluid">
							
															</div>
														</div>
													</div>
										</div>
								
					
					
		
		
				
		
		
		
						






				
										<div class="section-top-border">
											<br>
											<h3 style="color:MediumBlue "class="text-heading title_color">15. M-Training Toolkit on Polio/Routine Immunization for CMCs in CGPP</h3>
											<div class="portfolio_details_inner">
												<div class="row">
													<div class="offset-lg-1 col-lg-5">
														  <div class="portfolio_right_text mt-30">
															  <h5 class="text-uppercase">ZMQ SOFTWARE SYSTEMS, Gurgaon, India </h5>
																   <p>The primary strategy to interrupt transmission of wild poliovirus in India is to improve supplemental immunization activities and routine immunization coverage in priority districts with a focus on 107 high-risk blocks of western Uttar Pradesh and central Bihar.</p>
																	<ul class="list">
																	 <li><span>Duration </span>: January to July 2013																	 </li>
																	 <li><span>Technology</span>: Android and J2ME App

																	 </li>
																	 <li><span>Application</span>: Immunization, Building Health system’s 

																	 </li>
																	 <li><span>Interface</span>: GUI - Android App and J2ME App

																	 </li>
																	 <li><span>Dataset</span>: Address and People</li>
																	
							
																	 </li>
																	 </ul>
																	
															</div>
													  </div>
															 <div class="col-lg-6">
															<div class="left_img">
															   <img class="img-fluid" src="img/project/45.png" alt="">
															
															</div>
															<div class="button-group-area mt-40"> <br>
																<a href="#" class="genric-btn link circle">Github Link</a>
																<a href="#" class="genric-btn link circle">Research paper Link</a>
																</div>
														</div>
												 </div>
														  <div class="row">
															  <<img src="img/project/47.png" alt="" class="img-fluid">
															  <div class="col-md-4 mt-sm-20 left-align-p">
																 <p>Villages or urban areas with a history of wild poliovirus transmission, or hard-to-reach or resistant populations are categorized as high-risk areas within blocks. The Social Mobilization Network (SM Net) is formed in Uttar Pradesh to support polio eradication efforts through improved planning, implementation and monitoring of social mobilization activities in those high-risk areas.</p>
															   </div>
														   </div>
											 </div>
														
														<div class="row">
															<div class="col-md-4">
																<div class="single-defination">
																	<h4 class="mb-20">Results</h4>
																	<p>Vaccination outcomes in SM Net areas were as high as or higher than in non-SM Net areas. There was considerable variation in vaccination outcomes between districts.</p>
																</div>
															</div>
															<div class="col-md-4">
																<div class="single-defination">
																	<h4 class="mb-20">Interface</h4>
																	<img src="img/project/48.png" alt="" class="img-fluid">
																</div>
															</div>
															<div class="col-md-4">
																<div class="single-defination">
																	<h4 class="mb-20">Hardware Interface</h4>
																	<img src="img/project/46.png" alt="" class="img-fluid">
								
																</div>
															</div>
														</div>
											</div>
									
						
						
			
			
							
			
			
			
	
	




				
												<div class="section-top-border">
													<br>
													<h3 style="color:MediumBlue "class="text-heading title_color">16. Automatic Wireless Eye Monitoring System</h3>
													<div class="portfolio_details_inner">
														<div class="row">
															<div class="offset-lg-1 col-lg-5">
																  <div class="portfolio_right_text mt-30">
																	  <h5 class="text-uppercase">Essilor India Private Limited, New Delhi </h5>
																		   <p>Wireless communication system is designed and developed for remote patient monitoring for Instant Data Collection, Content development and better client interaction</p>
																			<ul class="list">
																			 <li><span>Duration </span>: 19.06.2012 to 31.07.2012  </li>
																			 <li><span>Technology</span>: IoT, Web Development</li>
																			 <li><span>Application</span>: Remote monitoring the condition of patient </li>
																			 <li><span>Interface</span>: Keil compiler,Embedded C</li>
																			 <li><span>Dataset</span>: Instant Data Collection
																				<li><span>Hardware</span>: 8051 series Microcontrollers, Encoder IC, Decoder IC, LCD, Transformer, Voltage Regulator, RF Module etc. </li>
																			 </ul>
																			
																	</div>
															  </div>
																	 <div class="col-lg-6">
																	<div class="left_img">
																	   <img class="img-fluid" src="img/project/51.jfif" alt="">
																	
																	</div>
																	<div class="button-group-area mt-40"> <br>
																		<a href="#" class="genric-btn link circle">Github Link</a>
																		<a href="#" class="genric-btn link circle">Research paper Link</a>
																		</div>
																</div>
														 </div>
																 
													 </div>
																
																<div class="row">
																	<div class="col-md-4">
																		<div class="single-defination">
																			<h4 class="mb-20">Results</h4>
																			<div class="col-md-3"><video width="280" height="220" controls><source src="img/project/13.mp4" type="video/mp4"></video></div>
																		</div>
																	</div>
																	<div class="col-md-4">
																		<div class="single-defination">
																			<h4 class="mb-20">Hardware Interface</h4>
																			<img src="img/project/49.jpeg" alt="" class="img-fluid">
																		</div>
																	</div>
																	<div class="col-md-4">
																		<div class="single-defination">
																			<h4 class="mb-20">Interface</h4>
																			<img src="img/project/50.png" alt="" class="img-fluid">
										
																		</div>
																	</div>
																</div>
													</div>
											
								
								
				










				
													<div class="section-top-border">
														<br>
														<h3 style="color:MediumBlue "class="text-heading title_color">17. Energy consumption optimization for Smart Classroom using Computer Vision</h3>
														<div class="portfolio_details_inner">
															<div class="row">
																<div class="offset-lg-1 col-lg-5">
																	  <div class="portfolio_right_text mt-30">
																		  <h5 class="text-uppercase">AI Project, Manipal University Jaipur, India </h5>
																			   <p>The goals are to reduce the unnecessary energy consumption of a smart home or office or classroom.</p>
																				<ul class="list">
																				 <li><span>Duration </span>: July 2019 to Dec 2019</li>
																				 <li><span>Technology</span>: Computer Vision</li>
																				 <li><span>Application</span>: Smart Homes, Smart Lighting Control																				 </li>
																				 <li><span>Interface</span>: Smartphone App</li>
																				 <li><span>Hardware</span>: Raspberry pi, Camera, Resistor, Breadboard, Relay</li></li>
																				 </ul>
																				 <div class="button-group-area mt-40"> <br>
																					<a href="#" class="genric-btn link circle">Github Link</a>
																					<a href="#" class="genric-btn link circle">Research paper Link</a>
																					</div>
																		</div>
																  </div>
																		 <div class="col-lg-6">
																		<div class="left_img">
																		   <img class="img-fluid" src="img/project/54.jpeg" alt="">
																		
																		</div>
																	</div>
															 </div>
																	  <div class="row">
																		  <div class="col-md-3"><video width="280" height="220" controls><source src="img/project/14.mp4" type="video/mp4"></video></div>
																		  <div class="col-md-9 mt-sm-20 left-align-p">
																			 <p>Computer Vision are used in this work to implement a smart home control network. The goals are to reduce the unnecessary energy consumption of a smart home or office or classroom. Computer vision with one coordinator, which is integrated into Raspberry pi, is established in each room. The coordinator is responsible for transferring environmental parameters obtained by WSNs to the management station. The control messages for home appliances are directly transferred using WSNs. Analysis of the illumination of a fluorescent lamp along with coordinates delay for even more energy conserving. </p>
																		   </div>
																	   </div>
														 </div>
																	
																	<div class="row">
																		<div class="col-md-4">
																			<div class="single-defination">
																				<h4 class="mb-20">Results</h4>
																				<p>The energy saving of lighting systems relative to those without smart control was evaluated. Numerical results indicate that the electricity consumption can be reduced by at least 40% under the smart control. Moreover, a prototype for the proposed smart home control network with illumination of light was implemented. Experimental tests demonstrate that the proposed system for smart home control networks is practically feasible and performs well.</p>
																			</div>
																		</div>
																		<div class="col-md-4">
																			<div class="single-defination">
																				<h4 class="mb-20">Hardware Interface</h4>
																				<img src="img/project/53.png" alt="" class="img-fluid">
																			</div>
																		</div>
																		<div class="col-md-4">
																			<div class="single-defination">
																				<h4 class="mb-20">Results</h4>
																				<img src="img/project/52.jpeg" alt="" class="img-fluid">
											
																			</div>
																		</div>
																	</div>
														</div>
												
									
									
							
							
							






				
														<div class="section-top-border">
															<br>
															<h3 style="color:MediumBlue "class="text-heading title_color">18. Unmanned Autonomous Grounded Surveillance Rover</h3>
															<div class="portfolio_details_inner">
																<div class="row">
																	<div class="offset-lg-1 col-lg-5">
																		  <div class="portfolio_right_text mt-30">
																			  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India  </h5>
																				   <ul class="list">
																					<p>The inevitability of robots is fundamentally a result of the need to reduce manpower requirements in military organizations, while  sustaining the capability needs of the organizations. The need to reduce manpower is driven by the trends of the macro environment for military organizations, which include the  aging demographics and reducing public  tolerance to human casualties. The evolving operational environment that places increasing demands on soldiers and exposes the limitations of the human body also supports the adoption of robots.</p>
																					 <li><span>Duration </span>: Jan 2016 – Dec 2017																					 </li>
																					 <li><span>Technology</span>: IoT, Computer Vision, Artificial Intelligence</li>
																					 <li><span>Application</span>: Surveillance for military or pedestrian 

																					 </li>
																					 <li><span>Interface</span>: Arduino, Raspberry pi </li>
																					 <li><span>Hardware</span>: Arduino, Raspberry pi, L293D Motor Driver Shield, IR sensors, DC Motors, Robot chassis, tyres and connectors</li></li>
																					 </ul>
																					 <div class="button-group-area mt-40"> <br>
																						<a href="#" class="genric-btn link circle">Github Link</a>
																						<a href="#" class="genric-btn link circle">Research paper Link</a>
																						</div>
																			</div>
																	  </div>
																			 <div class="col-lg-6">
																			<div class="left_img">
																			   <img class="img-fluid" src="img/project/58.jpeg" alt="">
																			
																			</div>
																		</div>
																 </div>
																		  <div class="row">
																			  <div class="col-md-3"><video width="280" height="220" controls><source src="img/project/16.mp4" type="video/mp4"></video></div>
																			  <div class="col-md-9 mt-sm-20 left-align-p">
																				 <p>The military forces of the future will use multi-agent robotic workforces for reconnaissance and surveillance, logistics and support, communications infrastructure, forward-deployed offensive  perations, and as tactical decoys to conceal maneuver by manned assets. Towards this end, there is a clear and definite need for optimal, multi-robot  control strategies in the synthesis, design, implementation, and fielding of autonomous and semi-autonomous teams of combat robots for military systems. Proven coordination methods are essential to  enable interactions within dynamic and hostile environments,  synchronized maneuvers, sensible and robust rules of engagement (ROE), and reliable field behavior.</p>
																			   </div>
																		   </div>
															 </div>
																		
																	
																		</div>
															</div>
													
										
										
								
								
								
	



				
															<div class="section-top-border">
																<br>
																<h3 style="color:MediumBlue "class="text-heading title_color">19. IoT based Surveillance Rover</h3>
																<div class="portfolio_details_inner">
																	<div class="row">
																		<div class="offset-lg-1 col-lg-5">
																			  <div class="portfolio_right_text mt-30">
																				  <h5 class="text-uppercase">IoT Project, Manipal University Jaipur, India  </h5>
																					   <ul class="list">
																						<p>Most of the application systems in the industry are designed in such a way that they give outputs in accordance with the predefined  conditions. To overcome this problem, I have proposed a method for autonomous control and decision making and reporting system, these types of mini robots contains self-neural schema framework for autonomous control. The robot is expected to navigate through the maze, that is, the robot is expected to avoid the obstacles while trying to find its way out. This project also considers camera for extended view. </p>
																						 <li><span>Duration </span>: Jan 2016 – Dec 2017																					 </li>
																						 <li><span>Technology</span>: IoT, Computer Vision, Artificial Intelligence</li>
																						 <li><span>Application</span>: Surveillance for military or pedestrian 
	
																						 </li>
																						 <li><span>Interface</span>: Arduino, Raspberry pi </li>
																						 <li><span>Hardware</span>: Arduino, Raspberry pi, L293D Motor Driver Shield, IR sensors, DC Motors, Robot chassis, tyres and connectors</li></li>
																						 </ul>
																						 <div class="button-group-area mt-40"> <br>
																							<a href="#" class="genric-btn link circle">Github Link</a>
																							<a href="#" class="genric-btn link circle">Research paper Link</a>
																							</div>
																				</div>
																		  </div>
																				 <div class="col-lg-6">
																				<div class="left_img">
																				   <img class="img-fluid" src="img/project/61.jpeg" alt="">
																				
																				</div>
																			</div>
																	 </div>
																			  <div class="row">
																				  <div class="col-md-3"><video width="280" height="220" controls><source src="img/project/17.mp4" type="video/mp4"></video></div>
																				  <div class="col-md-9 mt-sm-20 left-align-p">
																					 <p>The inevitability of robots is fundamentally a result of the need to reduce manpower requirements in military organizations, while  sustaining the capability needs of the organizations. The need to reduce manpower is driven by the trends of the macro environment for military organizations, which include the  aging demographics and reducing public  tolerance to human casualties. The evolving operational environment that places increasing demands on soldiers and exposes the limitations of the human body also supports the adoption of robots.</p>
																				   </div>
																			   </div>
																 </div>
																			
																		
																			</div>
																</div>
														
											
											
									
	
	
	
	
	
	
	
	























        </div>
    </section>
    <!--================End Portfolio Details Area =================-->


	
  































          
    <!--================Footer Area =================-->
	<footer class="footer_area">
        <div class="container">
            <div class="row justify-content-center">
                <div class="col-lg-12">
                    <div class="footer_top flex-column">
                        <div class="footer_logo">
                            <a href="#">
                                <i class="fa fa-chevron-up" aria-hidden="true"></i>
                            </a>
                            <h4>Follow Me</h4>
                        </div>
                        <div class="footer_social">
                            <a href="https://jaipur.manipal.edu/foe/schools-faculty/faculty-list/Anubha-parashar.html" target="_blank"><i class="fa fa-university" aria-hidden="true"></i></a>
							<a href="https://www.linkedin.com/in/anubhaparashar/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
							<a href="https://github.com/anubhaparashar" target="_blank"><i class="fa fa-git" aria-hidden="true"></i></a>
							<a href="https://www.facebook.com/anubha.parashar/" target="_blank"><i class="fa fa-facebook"></i></a>
							<a href="https://www.instagram.com/anubha_parashar/" target="_blank"><i class="fa fa-instagram" aria-hidden="true"></i></i></a>
							<a href="https://twitter.com/parashar_anubha" target="_blank"><i class="fa fa-twitter"></i></a>
							<a href="https://www.youtube.com/channel/UCmC85DV_GWSe3scjD-PcRhg" target="_blank"><i class="fa fa-youtube" aria-hidden="true"></i></a>
							<a href="https://in.pinterest.com/anubhaparashar1025/" target="_blank"><i class="fa fa-pinterest-square" aria-hidden="true"></i> </a>
							<a href="live:a199d70eb5cb2d04" target="_blank"><i class="fa fa-skype" aria-hidden="true"></i> live:a199d70eb5cb2d04</a>
							
							

							

                           
							
							<a href="https://scholar.google.com/citations?user=hrwpIAgAAAAJ&hl=en" target="_blank"><img src="img/icons/1.png" alt=""> </a>
							
                            <br>
							
							<a href="https://www.scopus.com/authid/detail.uri?authorId=57191284351" target="_blank"><img src="img/icons/2.png" alt=""></a>

							<a href="https://orcid.org/my-orcid?orcid=0000-0002-8474-3623" target="_blank"><img src="img/icons/3.png" alt=""></a>
							
							
							
							<a href="https://www.webofscience.com/wos/author/record/L-7545-2017" target="_blank"><img src="img/icons/4.png" alt=""></a>
							<a href="https://medium.com/@anubhaparashar1025" target="_blank"><img src="img/icons/5.png" alt=""></a>
							<a href="https://www.semanticscholar.org/author/Anubha-Parashar/2714843" target="_blank"><img src="img/icons/6.png" alt=""></a>
							<a href="https://muj.academia.edu/AnubhaParashar" target="_blank"><img src="img/icons/7.png" alt=""></a>
							
							
							<a href="https://www.researchgate.net/profile/Anubha-Parashar" target="_blank"><img src="img/icons/8.png" alt=""></a>
							
                            <a href="https://anubhaparashar.blogspot.com/" target="_blank"><img src="img/icons/9.png" alt=""></a>
							<a href="http://cooltechnoupdates.blogspot.com/" target="_blank"><img src="img/icons/9.png" alt=""></a>
							<a href="https://cool-computer-tricks-n-tips.blogspot.com/" target="_blank"><img src="img/icons/9.png" alt=""></a>
							
							
							<br>
							
							<a href="mailto:dranubhaparashar@gmail.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i> dranubhaparashar@gmail.com</a>
							<a href="mailto:anubhaparashar1025@gmail.com"- target="_blank"><i class="fa fa-envelope-o" aria-hidden="true"></i> anubhaparashar1025@gmail.com </a>
							<a href="mailto:anubha.parashar@jaipur.manipal.edu" target="_blank"><i class="fa fa-envelope-square" aria-hidden="true"></i> anubha.parashar@jaipur.manipal.edu </a>


							

							
							
                        </div>
                    </div>
                </div>
            </div>
            <div class="row footer_bottom justify-content-center">
                <p class="col-lg-8 col-sm-12 footer-text">
                   
            </div>
        </div>
    </footer>
    <!--================End Footer Area =================-->
    
		   
		<!-- Optional JavaScript -->
		<!-- jQuery first, then Popper.js, then Bootstrap JS -->
		<script src="js/jquery-3.2.1.min.js"></script>
		<script src="js/popper.js"></script>
		<script src="js/bootstrap.min.js"></script>
		<script src="js/stellar.js"></script>
		<script src="vendors/lightbox/simpleLightbox.min.js"></script>
		<script src="vendors/nice-select/js/jquery.nice-select.min.js"></script>
		<script src="vendors/isotope/imagesloaded.pkgd.min.js"></script>
		<script src="vendors/isotope/isotope-min.js"></script>
		<script src="vendors/owl-carousel/owl.carousel.min.js"></script>
		<script src="js/jquery.ajaxchimp.min.js"></script>
		<script src="js/mail-script.js"></script>
		<script src="js/theme.js"></script>
	</body>
</html>